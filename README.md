<div align="center"><h1>AI Chip (ICs and IPs)</h1></div>
<div align="center">Editor <a href="https://www.linkedin.com/in/shan-tang-27342510/"><strong>S.T.</strong></a>(Linkedin)</div>
<div align="center"><strong>Welcome to My Weichat Blog <a href="https://mp.weixin.qq.com/s/axfIBbQBDhTJ2Zt7U5WQBw">StarryHeavensAbove for more AI chip related articles</a></strong></div>
<div align="center"><h1> </h1></div>
<br>
<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/AI_Chip_Landscape_MidRes.png"></div>
<div align="center"><h1> </h1></div>
 
<div align="center"><h2>Latest updates</h2></div>
<HR>

<font color="Darkred">
<ul>
<li>Remove DeepScale who was acquired by Tesla.</li> 
<li>Add news of <a href="#Alibaba">Alibaba's cloud inference chip Hanguang 800</a>.</li>
<li>Add startup <a href="#Luminous">Luminous Computing</a>, <a href="#Efinix">Efinix</a>, <a href="#AIstorm">AISTORM</a>.</li>
<li>Add news of <a href="#Nvidia">NVDLA Deep Learning Inference Compiler</a>.</li>
<li>Add news of <a href="#Intel">Intel's AI chip</a>.</li>
<li>Add news of <a href="#Cerebras">Cerebras's huge WSE AI chip</a>.</li>
<li>Add startup <a href="#Neuroblade">Neuroblade</a>.</li>
<li>Add EEMBC MLMark Benchmark in  <a href="#AIChipBenchmarks">AI Chip Benchmarks</a>.</li>
<li>Add news of <a href="#InnoGrit">InnoGrit</a>.</li>
<li>Add startup <a href="#Areanna">Areanna AI</a>.</li>
<li>Add links to <a href="#Tsingmicro">TSING MICRO</a>,  <a href="#Ambarella">Ambarella</a> and <a href="#Blacksesame">Black Sesame</a>.</li>
<li>Add Chinese startup <a href="#WITINMEM">WITINMEM</a>.</li>
<li><a href="#AIChipBenchmarks">MLPerf v0.5 Inference Benchmarks are here</a>.</li>
<li>Add news of <a href="#Renesas_Electronics">Renesas Electronics's PIM AI Chip</a>.</li>
<li>Add news of <a href="#Habana">Habana's Gaudi AI Training Chip</a>.</li>
<li>Add news of <a href="#LG">LG's AI Chip</a>.</li>
<li>Add <a href="#AIChipCompilers">The Tensor Algebra Compiler (taco)</a>.</li>
<li>Add news of <a href="#Achronix">Achronix’s Speedster7t FPGA</a>.</li>
<li>Add news of <a href="#Hailo">Hailo’s Hailo-8 chip</a>.</li>
<li>Add MLIR in <a href="#AIChipCompilers">AI Compiler section</a> and news of <a href="#Tesla">Tesla's FSD chip.</a>.</li>
<li>Add startup <a href="#etacompute">Eta Compute</a>.</li>
<li>Add Compiler <a href="#AIChipCompilers">ONNC</a>.</li>
<li>Add Chinese startup <a href="#EEasy">EEasy Tech</a>.</li>
<li>Add startup <a href="#Optalysys">Optaylsys</a>.</li>
<li>Add a new section of <a href="#AIChipCompilers">AI Chip Compilers</a>.</li>
<li>Add Chinese startup <a href="#Enflame">Enflame</a>.</li>
<li>Add news of <a href="#Google">Google Edge TPU</a>.</li>
<li>Add a new section of <a href="#AIChipBenchmarks">AI Chip Benchmarks</a>.</li>
<li>Add news of <a href="#Horizon_Robotics">Horizon Robotics</a>.</li>
<li>Add startup <a href="#Anaflash">Anaflash</a>.</li>
<li>Add Chinese startup <a href="#Canaan">Canaan</a>.</li>
<li>Add startup <a href="#Cornami">Cornami</a>.</li>
<li>Add startup <a href="#PFN">Preferred Network</a>.</li>
<li>Add news of <a href="#MediaTek">Mediatek's Helio P90</a>.</li>
<li>Add <a href="#Reference">"White Paper on AI Chip Technologies 2018" in Reference</a>.</li>
<li>Add news of <a href="#Reference">MLPerf in Reference</a>.</li>
<li>Add startup <a href="#Flexlogix">Flex Logix</a>.</li>
<li>Add news of <a href="#Qualcomm">Qualcomm's Snapdragon 855 Mobile Platform</a>.</li>
<li>Add startup <a href="#aiCTX">aiCTX</a>.</li>
</ul>
</font>

<div align="center"><h1> </h1></div>

<div align="center"><h2>Shortcut</h2></div>
<HR>
<style>
table, th, td {
    border: 1px solid black;
}
</style>
<table style="width:100%">
  <tr>
    <th><a href="#IC_Vendors">IC Vendors</a></th><td><a href="#Intel">Intel</a>, <a href="#Qualcomm">Qualcomm</a>, <a href="#Nvidia">Nvidia</a>, <a href="#Samsung">Samsung</a>, <a href="#AMD">AMD</a>, <a href="#Xilinx">Xilinx</a>, <a href="#IBM">IBM</a>, <a href="#STMicroelectronics">STMicroelectronics</a>, <a href="#NXP">NXP</a>, <a href="#Marvell">Marvell</a>, <a href="#MediaTek">MediaTek</a>, <a href="#HiSilicon">HiSilicon</a>, <a href="#Rockchip">Rockchip</a>, <a href="#Renesas_Electronics">Renesas Electronics</a>, <a href="#Ambarella">Ambarella</a></td><td>15</td>
  </tr>
  <tr>
    <th><a href="#Tech_Giants">Tech Giants & HPC Vendors</a></th><td><a href="#Google">Google</a>, <a href="#Amazon_AWS">Amazon_AWS</a>, <a href="#Microsoft">Microsoft</a>, <a href="#Apple">Apple</a>, <a href="#Alibaba_Cloud">Aliyun</a>, <a href="#Alibaba">Alibaba Group</a>, <a href="#Tencent_Cloud">Tencent Cloud</a>, <a href="#Baidu">Baidu</a>, <a href="#Baidu_Cloud">Baidu Cloud</a>, <a href="#HUAWEI">HUAWEI</a>, <a href="#Fujitsu">Fujitsu</a>, <a href="#Nokia">Nokia</a>, <a href="#Facebook">Facebook</a>, <a href="#HPE">HPE</a>, <a href="#Tesla">Tesla</a>, <a href="#LG">LG</a></td><td>15</td>
  </tr>
  <tr>
    <th><a href="#IP_Vendors">IP Vendors</a></th><td><a href="#ARM">ARM</a>, <a href="#Synopsys">Synopsys</a>, <a href="#Imagination">Imagination</a>, <a href="#CEVA">CEVA</a>, <a href="#Cadence">Cadence</a>, <a href="#VeriSilicon">VeriSilicon</a>, <a href="#Videantis">Videantis</a></td><td>7</td>
  </tr>
  <tr>
    <th><a href="#Startups_in_China">Startups in China</a></th>
    <td><a href="#Cambricon">Cambricon</a>, <a href="#Horizon_Robotics">Horizon Robotics</a>, <a href="#Bitmain">Bitmain</a>, <a href="#Chipintelli">Chipintelli</a>, <a href="#Thinkforce">Thinkforce</a>, <a href="#Unisound">Unisound</a>, <a href="#AISpeech">AISpeech</a>, <a href="#Rokid">Rokid</a>, <a href="#Nextvpu">NextVPU</a>, <a href="#Canaan">Canaan</a>, <a href="#Enflame">Enflame</a>, <a href="#Eesay">Eesay Tech</a>, <a href="#WITINMEM">WITINMEM</a>, <a href="#Tsingmicro">TSING MICRO</a>,  <a href="#Blacksesame">Black Sesame</a></td><td>15</td>
  </tr>
  <tr>  
    <th><a href="#Startups_Worldwide">Startups Worldwide</a></th>
    <td><a href="#Cerebras">Cerebras</a>, <a href="#Wave">Wave Computing</a>, <a href="#Graphcore">Graphcore</a>, <a href="#PEZY">PEZY</a>, <a href="#Tenstorrent">Tenstorrent</a>, <a href="#Thinci">ThinCI</a>, <a href="#Koniku">Koniku</a>, <a href="#Adapteva">Adapteva</a>, <a href="#Knowm">Knowm</a>, <a href="#Mythic">Mythic</a>, <a href="#Kalray">Kalray</a>, <a href="#Brainchip">BrainChip</a>, <a href="#Aimotive">AImotive</a>, <a href="#Leepmind">Leepmind</a>, <a href="#Krtkl">Krtkl</a>, <a href="#NovuMind">NovuMind</a>, <a href="#REM">REM</a>, <a href="#TERADEEP">TERADEEP</a>, <a href="#DEEP_VISION">DEEP VISION</a>, <a href="#Groq">Groq</a>, <a href="#KAIST_DNPU">KAIST DNPU</a>, <a href="#Kneron">Kneron</a>, <a href="#Esperanto">Esperanto Technologies</a>, <a href="#GTI">Gyrfalcon Technology</a>, <a href="#SambaNova">SambaNova Systems</a>, <a href="#GreenWaves">GreenWaves Technology</a>, <a href="#Lightelligence">Lightelligence</a>, <a href="#Lightmatter">Lightmatter</a>, <a href="#ThinkSilicon">ThinkSilicon</a>, <a href="#Innogrit">Innogrit</a>, <a href="#Kortiq">Kortiq</a>, <a href="#Hailo">Hailo</a>,<a href="#Tachyum">Tachyum</a>,<a href="#Alphaics">AlphaICs</a>,<a href="#Syntiant">Syntiant</a>, <a href="#Habana">Habana</a>, <a href="#aiCTX">aiCTX</a>, <a href="#Flexlogix">Flex Logix</a>, <a href="#PFN">Preferred Network</a>, <a href="#Cornami">Cornami</a>, <a href="#Anaflash">Anaflash</a>, <a href="#Optalysys">Optaylsys</a>, <a href="#etacompute">Eta Compute</a>, <a href="#Achronix">Achronix</a>, <a href="#Areanna">Areanna AI</a>, <a href="#Neuroblade">Neuroblade</a>, <a href="#Luminous">Luminous Computing</a>, <a href="#Efinix">Efinix</a>, <a href="#AIstorm">AISTORM</a></td><td>49</td>
  </tr>
</table>

<div align="center"><h1> </h1></div>

<div align="center"><h2>Application Category</h2></div>
<HR>
<style>
table, th, td {
    border: 1px solid black;
}
</style>
<table style="width:100%">
  <tr>
    <th>Both</th>
    <th>Datacenter</th> 
    <th>Edge/Terminal</th>
  </tr>
  <tr>
    <td><a href="#Intel">Intel</a>, <a href="#Nvidia">Nvidia</a>, <a href="#IBM">IBM</a>, <a href="#Xilinx">Xilinx</a>, <a href="#HiSilicon">HiSilicon</a>, <a href="#Google">Google</a>, <a href="#Baidu">Baidu</a>, <a href="#Alibaba">Alibaba Group</a>, <a href="#Cambricon">Cambricon</a>, <a href="#Bitmain">Bitmain</a>, <a href="#Wave">Wave Computing</a>,<a href="#Tachyum">Tachyum</a>,<a href="#Alphaics">AlphaICs</a>, <a href="#Marvell">Marvell</a>, <a href="#Achronix">Achronix</a></td>  
    <td><a href="#AMD">AMD</a>, <a href="#Microsoft">Microsoft</a>, <a href="#Apple">Apple</a>, <a href="#Tencent_Cloud">Tencent Cloud</a>,<a href="#Alibaba_Cloud">Aliyun</a>, <a href="#Baidu_Cloud">Baidu Cloud</a>, <a href="#HUAWEI">HUAWEI</a>, <a href="#Fujitsu">Fujitsu</a>, <a href="#Nokia">Nokia</a>, <a href="#Facebook">Facebook</a>, <a href="#HPE">HPE</a>, <a href="#Thinkforce">Thinkforce</a>, <a href="#Cerebras">Cerebras</a>, <a href="#Graphcore">Graphcore</a>, <a href="#Groq">Groq</a>, <a href="#SambaNova">SambaNova Systems</a>, <a href="#Adapteva">Adapteva</a>, <a href="#PEZY">PEZY</a>, <a href="#Habana">Habana</a>, <a href="#Enflame">Enflame</a></td>
    <td><a href="#Qualcomm">Qualcomm</a>, <a href="#Samsung">Samsung</a>, <a href="#STMicroelectronics">STMicroelectronics</a>, <a href="#NXP">NXP</a>, <a href="#MediaTek">MediaTek</a>, <a href="#Tesla">Tesla</a>, <a href="#Rockchip">Rockchip</a>, <a href="#Amazon_AWS">Amazon_AWS</a>, <a href="#ARM">ARM</a>, <a href="#Synopsys">Synopsys</a>, <a href="#Imagination">Imagination</a>, <a href="#CEVA">CEVA</a>, <a href="#Cadence">Cadence</a>, <a href="#VeriSilicon">VeriSilicon</a>, <a href="#Videantis">Videantis</a>, <a href="#Horizon_Robotics">Horizon Robotics</a>, <a href="#Chipintelli">Chipintelli</a>, <a href="#Unisound">Unisound</a>, <a href="#AISpeech">AISpeech</a>, <a href="#Rokid">Rokid</a>, <a href="#Tenstorrent">Tenstorrent</a>, <a href="#Thinci">ThinCI</a>, <a href="#Koniku">Koniku</a>, <a href="#Knowm">Knowm</a>, <a href="#Mythic">Mythic</a>, <a href="#Kalray">Kalray</a>, <a href="#Brainchip">BrainChip</a>, <a href="#Aimotive">AImotive</a>, <a href="#Leepmind">Leepmind</a>, <a href="#Krtkl">Krtkl</a>, <a href="#NovuMind">NovuMind</a>, <a href="#REM">REM</a>, <a href="#TERADEEP">TERADEEP</a>, <a href="#DEEP_VISION">DEEP VISION</a>, <a href="#KAIST_DNPU">KAIST DNPU</a>, <a href="#Kneron">Kneron</a>, <a href="#Esperanto">Esperanto Technologies</a>, <a href="#GTI">Gyrfalcon Technology</a>, <a href="#GreenWaves">GreenWaves Technology</a>, <a href="#Lightelligence">Lightelligence</a>, <a href="#Lightmatter">Lightmatter</a>, <a href="#ThinkSilicon">ThinkSilicon</a>, <a href="#Innogrit">Innogrit</a>, <a href="#Kortiq">Kortiq</a>, <a href="#Hailo">Hailo</a>,<a href="#Syntiant">Syntiant</a>, <a href="#Nextvpu">NextVPU</a>, <a href="#aiCTX">aiCTX</a>, <a href="#Cornami">Cornami</a>, <a href="#Anaflash">Anaflash</a>, <a href="#Eesay">Eesay Tech</a>, <a href="#Optalysys">Optaylsys</a>, <a href="#etacompute">Eta Compute</a>, <a href="#LG">LG</a>, <a href="#Renesas_Electronics">Renesas Electronics</a>, <a href="#WITINMEM">WITINMEM</a>,  <a href="#Ambarella">Ambarella</a>, <a href="#Tsingmicro">TSING MICRO</a>,  <a href="#Blacksesame">Black Sesame</a>, <a href="#Areanna">Areanna AI</a>,  <a href="#Neuroblade">Neuroblade</a></td>
  </tr>
</table>

<div align="center"><h1> </h1></div>

<div align="center"><h3> </h3></div>


<div align="center"><h1> </h1></div>


<div align="center"><h2><a name="IC_Vendors"></a>I. IC Vendors</h2></div>
<HR>
<div align="center"><h1> </h1></div>

<div align="center"><h3> </h3></div>
<a name="Intel"></a>
<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Intel_logo.png" height="60"></div>
<div align="center"><h3> </h3></div>

<strong><a href="https://newsroom.intel.com/news/hot-chips-2019/">At Hot Chips, Intel Pushes ‘AI Everywhere’</a></strong>
> At Hot Chips 2019, Intel revealed new details of upcoming high-performance artificial intelligence (AI) accelerators: Intel® Nervana™ neural network processors, with the NNP-T for training and the NNP-I for inference. Intel engineers also presented technical details on hybrid chip packaging technology, Intel® Optane™ DC persistent memory and chiplet technology for optical I/O.
<br>

<a name="Mobileye"></a>
<div align="center"><h3>Mobileye EyeQ</h3></div>
> Mobileye is currently developing its fifth generation SoC, the <a href="https://www.mobileye.com/our-technology/evolution-eyeq-chip/">EyeQ®5</a>, to act as the vision central computer performing sensor fusion for Fully Autonomous Driving (Level 5) vehicles that will hit the road in 2020. To meet power consumption and performance targets, EyeQ® SoCs are designed in most advanced VLSI process technology nodes – down to 7nm FinFET in the 5th generation. 

<a name="Movidius"></a>
<div align="center"><h3>Movidius</h3></div>
<a href="https://pdfs.semanticscholar.org/32d5/405ac92a13d7f38e2313574dfd6238125a94.pdf">MYRIAD 2</a> IS A MULTICORE, ALWAYS-ON SYSTEM ON CHIP THAT SUPPORTS COMPUTATIONAL IMAGING AND VISUAL AWARENESS FOR MOBILE, WEARABLE, AND EMBEDDED APPLICATIONS. THE VISION PROCESSING UNIT INCORPORATES PARALLELISM, INSTRUCTION SET ARCHITECTURE, AND MICROARCHITECTURAL FEATURES TO PROVIDE HIGHLY SUSTAINABLE PERFORMANCE EFFICIENCY ACROSS A RANGE OF COMPUTATIONAL IMAGING AND COMPUTER VISION APPLICATIONS, INCLUDING THOSE WITH LOW LATENCY REQUIREMENTS ON THE ORDER OF MILLISECONDS.

<a href="https://www.movidius.com/myriadx">Myriad™ X</a> is the first VPU to feature the Neural Compute Engine - a dedicated hardware accelerator for running on-device deep neural network applications. Interfacing directly with other key components via the intelligent memory fabric, the Neural Compute Engine is able to deliver industry leading performance per Watt without encountering common data flow bottlenecks encountered by other architectures.

<a name="Intel_FPGA"></a>
<div align="center"><h3>FPGA</h3></div>
Intel <a href="https://www.altera.com/products/design-software/embedded-software-developers/opencl/overview.html">FPGA OpenCL</a> and <a href="https://www.altera.com/solutions/technology/artificial-intelligence/solutions.html">Solutions</a>.

<a name="Loihi"></a>
<div align="center"><h3>Loihi</h3></div>
Intel's Loihi test chip is the <a href="https://newsroom.intel.com/editorials/intels-new-self-learning-chip-promises-accelerate-artificial-intelligence/">First-of-Its-Kind Self-Learning Chip</a>.
> The Loihi research test chip includes digital circuits that mimic the brain’s basic mechanics, making machine learning faster and more efficient while requiring lower compute power. Neuromorphic chip models draw inspiration from how neurons communicate and learn, using spikes and plastic synapses that can be modulated based on timing. This could help computers self-organize and make decisions based on patterns and associations.

<div align="center"><h3> </h3></div>
<a name="Qualcomm"></a>
<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Qualcomm_logo.png" height="40"></div>
<div align="center"><h3> </h3></div>

<strong><a href="https://www.qualcomm.com/news/releases/2019/04/09/qualcomm-brings-power-efficient-artificial-intelligence-inference">Qualcomm Brings Power Efficient Artificial Intelligence Inference Processing to the Cloud</a></strong>
> Qualcomm Technologies, Inc., a subsidiary of Qualcomm Incorporated (NASDAQ: QCOM), announced that it is bringing the Company’s artificial intelligence (AI) expertise to the cloud with the Qualcomm® Cloud AI 100. Built from the ground up to meet the explosive demand for AI inference processing in the cloud, the Qualcomm Cloud AI 100 utilizes the Company’s heritage in advanced signal processing and power efficiency. <br>


<a href="https://www.qualcomm.com/products/snapdragon-855-mobile-platform">Snapdragon 855 Mobile Platform</a>
> Our 4th generation on-device AI engine is the ultimate personal assistant for camera, voice, XR and gaming – delivering smarter, faster and more secure experiences. Utilizing all cores, it packs 3 times the power of its predecessor for stellar on-device AI capabilities.
> Greater than 7 trillion operations per second (TOPS)

<div align="center"><h3> </h3></div>
<a name="Nvidia"></a>
<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Nvidia_logo.png" height="50"> </div>
<div align="center"><h3> </h3></div>
<div align="center"><h3>GPU</h3></div>

<strong><a href="https://devblogs.nvidia.com/nvdla/">NVDLA Deep Learning Inference Compiler is Now Open Source</a></strong>
> With the open-source release of NVDLA’s optimizing compiler on <a href="https://github.com/nvdla/sw/releases/tag/v1.2.0-OC">GitHub</a>, system architects and software teams now have a starting point with the complete source for the world’s first fully open software and hardware inference platform. 

<strong><a href="https://www.nvidia.com/en-us/data-center/tesla-t4/">NVIDIA TESLA T4 TENSOR CORE GPU</a></strong>
> Powering the TensorRT Hyperscale Inference Platform.

<strong><a href="https://www.anandtech.com/show/13214/nvidia-reveals-next-gen-turing-gpu-architecture">NVIDIA Reveals Next-Gen Turing GPU Architecture: NVIDIA Doubles-Down on Ray Tracing, GDDR6, &amp; More</a></strong>
> at NVIDIA’s SIGGRAPH 2018 keynote presentation, company CEO Jensen Huang formally unveiled the company’s much awaited (and much rumored) Turing GPU architecture. The next generation of NVIDIA’s GPU designs, Turing will be incorporating a number of new features and is rolling out this year. 

<a href="https://www.nextplatform.com/2018/03/28/nvidia-dgx-2-system-packs-an-ai-performance-punch/">Nvidia’s DGX-2 System Packs An AI Performance Punch</a>

<a href="https://www.nextplatform.com/2018/04/13/building-bigger-faster-gpu-clusters-using-nvswitches/">Building Bigger, Faster GPU Clusters Using NVSwitches</a>
> Nvidia launched its second-generation DGX system in March. In order to build the 2 petaflops half-precision DGX-2, Nvidia had to first design and build a new NVLink 2.0 switch chip, named NVSwitch. While Nvidia is only shipping NVSwitch as an integral component of its DGX-2 systems today, Nvidia has not precluded selling NVSwitch chips to data center equipment manufacturers.

<a href="https://www.nvidia.com/en-us/data-center/tesla-v100/">Nvidia's latest GPU</a> can do 15 TFlops of SP or 120 TFlops with its new Tensor core architecture which is a FP16 multiply and FP32 accumulate or add to suit ML.<br>
<br>
Nvidia is packing up 8 boards into their <a href="https://www.nvidia.com/en-us/data-center/dgx-server/">DGX-1</a>for 960 Tensor TFlops.<br><br>
<a href="https://mp.weixin.qq.com/s/tEX4H7OEbZF4dKMI0ZOPmw">Nvidia Volta - 架构看点</a> gives some insights of Volta architecture.<br><br>

<div align="center"><h3>SoC</h3></div>
On edge, Nvidia provide NVIDIA DRIVE™ PX, <a href="http://www.nvidia.com/object/drive-px.html">The AI Car Computer for Autonomous Driving</a> and JETSON TX1/TX2 MODULE, "<a href="http://www.nvidia.com/object/embedded-systems-dev-kits-modules.html">The embedded platform for autonomous everything</a>".

<div align="center"><h3>NVDLA</h3></div>
Nvidia anouced "XAVIER DLA NOW OPEN SOURCE" on GTC2017. We did not see Early Access verion yet. Hopefully, the general release will be avaliable on Sep. as promised. For more analysis, you may want to read <a href="http://mp.weixin.qq.com/s/XEb5xNeSV_oPs08kDgQg8Q">从Nvidia开源深度学习加速器说起</a>.

<br> 
Now the open source DLA is available on <a href="https://github.com/nvdla/">Github</a> and more information can be found <a href="http://nvdla.org/">here</a>.
> The NVIDIA Deep Learning Accelerator (NVDLA) is a free and open architecture that promotes a standard way to design deep learning inference accelerators. With its modular architecture, NVDLA is scalable, highly configurable, and designed to simplify integration and portability. The hardware supports a wide range of IoT devices. Delivered as an open source project under the NVIDIA Open NVDLA License, all of the software, hardware, and documentation will be available on GitHub. Contributions are welcome.

<div align="center"><h3> </h3></div>
<a name="Samsung"></a>
<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Samsung_logo.png" height="35"></div>
<div align="center"><h3> </h3></div>
<strong><a href="https://news.samsung.com/global/samsung-brings-on-device-ai-processing-for-premium-mobile-devices-with-exynos-9-series-9820-processor">Samsung Brings On-device AI Processing for Premium Mobile Devices with Exynos 9 Series 9820 Processor</a></strong>
> Fourth-generation custom core and 2.0Gbps LTE Advanced Pro modem enables enriched mobile experiences including AR and VR applications 

<br> 
Samsung resently unveiled “<a href="https://news.samsung.com/global/samsung-optimizes-premium-exynos-9-series-9810-for-ai-applications-and-richer-multimedia-content">The new Exynos 9810 brings premium features with a 2.9GHz custom CPU, an industry-first 6CA LTE modem and deep learning processing capabilities</a>”.   

<div align="center"><h3> </h3></div>
<a name="AMD"></a>
<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/AMD_logo.png" height="35"></div>
<div align="center"><h3> </h3></div>
The soon to be released <a href="https://instinct.radeon.com/en-us/product/mi/radeon-instinct-mi25/">AMD Radeon Instinct MI25</a> is promising 12.3 TFlops of SP or 24.6 TFlops of FP16. If your calculations are amenable to Nvidia's Tensors, then AMD can't compete. Nvidia also does twice the bandwidth with 900GB/s versus AMD's 484 GB/s.
> AMD has put <a href="https://www.nextplatform.com/2017/08/08/shape-amd-hpc-ai-iron-come/">a very good X86 server processor</a> into the market for the first time in nine years, and it also has a matching GPU that gives its OEM and ODM partners a credible alternative for HPC and AI workload to the combination of Intel Xeons and Nvidia Teslas that dominate hybrid computing these days.

> <a href="https://techcrunch.com/2017/09/20/tesla-said-to-be-working-on-its-own-self-driving-ai-chip-with-amd/">Tesla is reportedly developing its own processor for artificial intelligence, intended for use with its self-driving systems, in partnership with AMD</a>. Tesla has an existing relationship with Nvidia, whose GPUs power its Autopilot system, but this new in-house chip reported by CNBC could potentially reduce its reliance on third-party AI processing hardware.

<div align="center"><h3> </h3></div>
<a name="Xilinx"></a>
<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Xilinx_logo.png" height="35"></div>
<div align="center"><h3> </h3></div>

<strong><a href="https://www.xilinx.com/news/press/2018/xilinx-launches-the-world-s-fastest-data-center-and-ai-accelerator-cards.html">Xilinx Launches the World's Fastest Data Center and AI Accelerator Cards</a></strong>
> Xilinx launched [Alveo](https://www.xilinx.com/alveo), a portfolio of powerful accelerator cards designed to dramatically increase performance in industry-standard servers across cloud and on-premise data centers. 

Xilinx provide "<a href="https://www.xilinx.com/applications/megatrends/machine-learning.html">Machine Learning Inference Solutions from Edge to Cloud</a>" and naturally claim their FPGA's are best for INT8 with one of their <a href="https://www.xilinx.com/support/documentation/white_papers/wp486-deep-learning-int8.pdf">white papers</a>.

> Whilst performance per Watt is impressive for FPGAs, the vendors' larger chips have long had earth shatteringly high chip prices for the larger chips. Finding a balance between price and capability is the main challenge with the FPGAs.

<div align="center"><h3> </h3></div>

<p><a name="IBM"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/IBM_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><a href="http://www.research.ibm.com/articles/brain-chip.shtml">TrueNorth</a> is IBM's Neuromorphic CMOS ASIC developed in conjunction with the DARPA <a href="https://en.wikipedia.org/wiki/SyNAPSE">SyNAPSE</a> program.</p>

<blockquote>
  <p>It is a manycore processor network on a chip design, with 4096 cores, each one simulating 256 programmable silicon "neurons" for a total of just over a million neurons. In turn, each neuron has 256 programmable "synapses" that convey the signals between them. Hence, the total number of programmable synapses is just over 268 million (228). In terms of basic building blocks, its transistor count is 5.4 billion. Since memory, computation, and communication are handled in each of the 4096 neurosynaptic cores, TrueNorth circumvents the von-Neumann-architecture bottlenecks and is very energy-efficient, consuming 70 milliwatts, about 1/10,000th the power density of conventional microprocessors. <a href="https://en.wikipedia.org/wiki/TrueNorth">Wikipedia</a></p>
</blockquote>

<p><a href="https://www.ibm.com/blogs/systems/with-ibm-power9-were-ahead-of-the-ai-wave/">With IBM POWER9, we’re all riding the AI wave</a></p>

<blockquote>
  <p>"With POWER9, we’re moving to a new off-chip era, with advanced accelerators like GPUs and FPGAs driving modern workloads, including AI...POWER9 will be the first commercial platform loaded with on-chip support for NVIDIA’s next-generation NVLink, OpenCAPI 3.0 and PCI-Express 4.0. These technologies provide a giant hose to transfer data." </p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="STMicroelectronics"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/STMicroelectronics_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><a href="http://www.eenewseurope.com/news/st-preps-second-neural-network-ic-0">ST preps second neural network IC</a></p>

<blockquote>
  <p>STMicroelectronics is designing a second iteration of the neural networking technology that the company reported on at the International Solid-State Circuits Conference (ISSCC) in February 2017.  </p>
</blockquote>

<p><a href="http://mp.weixin.qq.com/s/POZ9k5INJC8SCKMNogzmXg">ISSCC2017 Deep-Learning Processors文章学习 （一）</a> is a reference.</p>

<div align="center"><h3> </h3></div>

<p><a name="NXP"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/NXP_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><strong>S32 AUTOMOTIVE PLATFORM</strong>
<br> <a href="https://www.nxp.com/products/processors-and-microcontrollers/arm-based-processors-and-mcus/s32-automotive-platform:S32">S32 AUTOMOTIVE PLATFORM</a></p>

<blockquote>
  <p>The NXP S32 automotive platform is the world’s first scalable automotive computing architecture. It offers a unified hardware platform and an identical software environment across application domains to bring rich in-vehicle experiences and automated driving functions to market faster.  </p>
</blockquote>

<p><strong>ADAS Chip</strong>
<br> <a href="https://www.nxp.com/products/processors-and-microcontrollers/arm-based-processors-and-mcus/s32-automotive-platform/vision-processor-for-front-and-surround-view-camera-machine-learning-and-sensor-fusion-applications:S32V234?lang_cd=en">S32V234: Vision Processor for Front and Surround View Camera, Machine Learning and Sensor Fusion Applications</a></p>

<blockquote>
  <p>The S32V234 is our 2nd generation vision processor family designed to support computation intensive applications for image processing and offers an ISP, powerful 3D GPU, dual APEX-2 vision accelerators, security and supports SafeAssure™. S32V234 is suited for ADAS, NCAP front camera, object detection and recognition, surround view, machine learning and sensor fusion applications. S32V234 is engineered for automotive-grade reliability, functional safety and security measures to support vehicle and industrial automation.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Marvell"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Marvell_logo.png" height="60"></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.marvell.com/company/news/pressDetail.do?releaseID=9976">Marvell Demonstrates Artificial Intelligence SSD Controller Architecture Solution</a></strong></p>

<blockquote>
  <p>Marvell will demonstrate today at the Flash Memory Summit how it will provide artificial intelligence capabilities to a broad range of industries by incorporating NVIDIA’s Deep Learning Accelerator (NVDLA) technology in its family of data center and client SSD controllers. </p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="MediaTek"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/MediaTek_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://i.mediatek.com/p90">MediaTek announced Helio P90, highlighting AI processing.</a></strong></p>

<p>This article, <strong><a href="https://www.anandtech.com/show/13718/mediatek-announces-new-premium-helio-p90-soc">"MediaTek Announces New Premium Helio P90 SoC"</a></strong>, from AnandTech has more in-deepth analysis. </p>

<div align="center"><h3> </h3></div>

<p><a name="HiSilicon"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/HiSilicon_logo.png" height="60"></div>

<div align="center"><h3> </h3></div>

<p><strong>Kirin for Smart Phone</strong>
<br> 
<strong><a href="https://consumer.huawei.com/en/campaign/kirin980/">Kirin 980, the World's First 7nm Process Mobile AI Chipset</a></strong></p>

<blockquote>
  <p>Introducing the Kirin 980, the world's first 7nm process mobile phone SoC chipset, the world’s first cortex-A76 architecture chipset, the world’s first dual NPU design, and the world’s first chipset to support LTE Cat.21. The Kirin 980 combines multiple technological inFtions and leads the AI trend to provide users with impressive mobile performance and to create a more convenient and intelligent life.</p>
</blockquote>

<p>HiSilicon <a href="http://consumer.huawei.com/minisite/worldwide/huawei-ifa2017-global-launch-event-presentation/">Kirin 970 Processor</a> annouced fearturing with dedicated Neural-network Processing Unit. <br />
In <a href="https://www.anandtech.com/show/11815/huawei-mate-10-and-mate-10-pro-launch-on-october-16th-more-kirin-970-details">this article</a>,we can find more details about NPU in Kirin970.</p>

<p><strong>Mobile Camera SoC</strong>
<br>According to a Brief Data Sheet of <a href="http://www.hisilicon.com/cn/-/media/Hisilicon/pdf/Surveillance_mobilecam/Hi3559A%20V100.pdf">Hi3559A V100ESultra-HD Mobile Camera SoC</a>, it has:</p>

<blockquote>
  <p>Dual-core CNN@700 MHz neural network acceleration engine</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Rockchip"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Rockchip_logo.png" height="50"></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.prnewswire.com/news-releases/rockchip-released-its-first-ai-processor-rk3399pro----npu-performance-up-to-24tops-300578633.html">Rockchip Released Its First AI Processor RK3399Pro -- NPU Performance up to 2.4TOPs</a></p>

<blockquote>
  <p>RK3399Pro adopted exclusive AI hardware design. Its NPU computing performance reaches 2.4TOPs, and indexes of both high performance and low consumption keep ahead: the performance is 150% higher than other same type NPU processor; the power consumption is less than 1%, comparing with other solutions adopting GPU as AI computing unit. </p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Renesas_Electronics"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Renesas_Electronics_logo.png" height="30"></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.renesas.com/eu/en/about/press-center/news/2019/news20190613.html">Renesas Electronics Develops New Processing-In-Memory Technology for Next-Generation AI Chips that Achieves AI Processing Performance of 8.8 TOPS/W</a></strong></p>

<blockquote>
  <p>Renesas Electronics Corporation (TSE: 6723), a premier supplier of advanced semiconductor solutions, today announced it has developed an AI accelerator that performs CNN (convolutional neural network) processing at high speeds and low power to move towards the next generation of Renesas embedded AI (e-AI), which will accelerate increased intelligence of endpoint devices. A Renesas test chip featuring this accelerator has achieved the power efficiency of 8.8 TOPS/W (Note 1), which is the industry's highest class of power efficiency. The Renesas accelerator is based on the processing-in-memory (PIM) architecture, an increasingly popular approach for AI technology, in which multiply-and-accumulate operations are performed in the memory circuit as data is read out from that memory. </p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Ambarella"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Ambarella_logo.png" height="50"></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.ambarella.com/">Intelligent Vision Processors For Edge Applications</a></p>

<div align="center"><h3> </h3></div>

<div align="center"><h2><a name="Tech_Giants"></a>II. Tech Giants & HPC Vendors</h2></div>

<p><HR></p>

<div align="center"><h3> </h3></div>

<p><a name="Google"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Google_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://venturebeat.com/2019/03/06/google-begins-selling-the-150-coral-dev-board-a-hardware-kit-for-accelerated-ai-edge-computing/">Google begins selling the $150 Coral Dev Board, a hardware kit for accelerated AI edge computing</a></strong></p>
<blockquote>
  <p>If you’re a software dev looking to get a head start on AI development at the edge, why not try on Google’s new hardware for size? The search company today made available the Coral Dev Board, a $150 computer featuring a removable system-on-module with one of its custom tensor processing unit (TPU) AI chips.</p>
</blockquote>

<p>Google's <a href="https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu">original TPU</a> had a big lead over GPUs and helped power DeepMind's AlphaGo victory over Lee Sedol in a Go tournament. The original 700MHz TPU is described as having 95 TFlops for 8-bit calculations or 23 TFlops for 16-bit whilst drawing only 40W. This was much faster than GPUs on release but is now slower than Nvidia's V100, but not on a per W basis. The new <a href="https://www.nextplatform.com/2017/05/22/hood-googles-tpu2-machine-learning-clusters/">TPU2</a> is referred to as a TPU device with four chips and can do around 180 TFlops. Each chip's performance has been doubled to 45 TFlops for 16-bits. You can see the gap to Nvidia's V100 is closing. You can't buy a TPU or TPU2. <br><br>
<a href="https://cloudplatform.googleblog.com/2018/02/Cloud-TPU-machine-learning-accelerators-now-available-in-beta.html">Lately</a>, Google is making <a href="https://cloud.google.com/tpu/">Cloud TPUs</a> available for use in <a href="https://cloud.google.com/">Google Cloud Platform (GCP)</a>. Here you can find the latest <a href="https://blog.riseml.com/benchmarking-googles-new-tpuv2-121c03b71384">banchmark result of Google TPU2</a>.<br><br>
<a href="https://www.blog.google/products/pixel/pixel-visual-core-image-processing-and-machine-learning-pixel-2/">Pixel Visual Core</a> is Google’s first custom-designed co-processor for consumer products. It’s built into every Pixel 2, and in the coming months, we’ll turn it on through a software update to enable more applications to use Pixel 2’s camera for taking HDR+ quality pictures.</p>

<p><a href="https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/">Tearing Apart Google’s TPU 3.0 AI Coprocessor</a></p>

<blockquote>
  <p>Google did its best to impress this week at its annual IO conference. While Google rolled out a bunch of benchmarks that were run on its current Cloud TPU instances, based on TPUv2 chips, the company divulged a few skimpy details about its next generation TPU chip and its systems architecture. The company changed from version notation (TPUv2) to revision notation (TPU 3.0) with the update, but ironically the detail we have assembled shows that the step from TPUv2 to what we will call TPUv3 probably isn’t that big; it should probably be called TPU v2r5 or something like that.</p>
</blockquote>

<p><strong><a href="https://cloud.google.com/edge-tpu/">Edge TPU</a></strong></p>

<blockquote>
  <p>AI is pervasive today, from consumer to enterprise applications. With the explosive growth of connected devices, combined with a demand for privacy/confidentiality, low latency and bandwidth constraints, AI models trained in the cloud increasingly need to be run at the edge. Edge TPU is Google’s purpose-built ASIC designed to run AI at the edge. It delivers high performance in a small physical and power footprint, enabling the deployment of high-accuracy AI at the edge.</p>
</blockquote>

<p>Other references are:<br>
<a href="https://mp.weixin.qq.com/s/b22p26_delWfSpy9kDJKhA">Google TPU3 看点</a><br><br>
<a href="https://mp.weixin.qq.com/s/Kf_L4u7JRxJ8kF3Pi8M5iw">Google TPU 揭密</a><br><br>
<a href="https://mp.weixin.qq.com/s/lBQyNSNa6-joeLZ_Kq2W8A">Google的神经网络处理器专利</a><br><br>
<a href="https://mp.weixin.qq.com/s/g-BDlvSy-cx4AKItcWF7jQ">脉动阵列 - 因Google TPU获得新生</a><br><br>
<a href="https://www.linkedin.com/pulse/should-we-all-embrace-systolic-arrays-chien-ping-lu">Should We All Embrace Systolic Arrays?</a><br></p>

<div align="center"><h3> </h3></div>

<p><a name="Amazon"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Amazon_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><a href="https://techcrunch.com/2018/02/12/amazon-may-be-developing-ai-chips-for-alexa/">Amazon may be developing AI chips for Alexa</a></p>

<blockquote>
  <p>The Information has a report this morning that Amazon <a href="https://www.theinformation.com/amazon-is-becoming-an-ai-chip-maker-speeding-alexa-responses?shared=922dfb3ba4e3984e">is working on building AI chips for the Echo</a>, which would allow Alexa to more quickly parse information and get those answers.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Amazon_AWS"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Amazon_AWS.png" height="50"></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://aws.amazon.com/cn/machine-learning/inferentia/">AWS Inferentia. High performance machine learning inference chip, custom designed by AWS.</a></strong></p>
<blockquote>
  <p>AWS Inferentia provides high throughput, low latency inference performance at an extremely low cost. Each chip provides hundreds of TOPS (tera operations per second) of inference throughput to allow complex models to make fast predictions. For even more performance, multiple AWS Inferentia chips can be used together to drive thousands of TOPS of throughput. AWS Inferentia will be available for use with Amazon SageMaker, Amazon EC2, and Amazon Elastic Inference.</p>
</blockquote>

<p><strong>AWS FPGA instance</strong></p>
<blockquote>
  <p><a href="https://aws.amazon.com/ec2/instance-types/f1/?nc1=h_ls">Amazon EC2 F1</a> is a compute instance with field programmable gate arrays (FPGAs) that you can program to create custom hardware accelerations for your application. F1 instances are easy to program and come with everything you need to develop, simulate, debug, and compile your hardware acceleration code, including an <a href="https://aws.amazon.com/marketplace/pp/B06VVYBLZZ">FPGA Developer AMI</a> and <a href="https://github.com/aws/aws-fpga">Hardware Developer Kit</a> (HDK). Once your FPGA design is complete, you can register it as an Amazon FPGA Image (AFI), and deploy it to your F1 instance in just a few clicks. You can reuse your AFIs as many times, and across as many F1 instances as you like.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Microsoft"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Microsoft_logo.png" height="60"></div>

<div align="center"><h3> </h3></div>

<p><a href="https://channel9.msdn.com/Events/Build/2017/B8063">Inside the Microsoft FPGA-based configurable cloud</a> is also a good reference if want to know Microsoft's vision on FPGA in cloud.<br><br>
<a href="http://mp.weixin.qq.com/s/Ti6N1SJ7UDRSQtl869Qvlg">This article "智慧云中的FPGA"</a> gives and overview about FPGA used in AI aceleration in the cloud.<br><br>
<a href="https://www.nextplatform.com/2017/08/24/drilling-microsofts-brainwave-soft-deep-leaning-chip/">Drilling Into Microsoft’s BrainWave Soft Deep Learning Chip</a> shows more details based on Microsoft's presentation on Hot Chips 2017.<br><br>
<a href="https://blogs.microsoft.com/ai/build-2018-project-brainwave/">Real-time AI: Microsoft announces preview of Project Brainwave</a></p>

<blockquote>
  <p>At Microsoft’s <a href="https://news.microsoft.com/2018/05/07/microsoft-build-highlights-new-opportunity-for-developers-at-the-edge-and-in-the-cloud/">Build developers conference in Seattle this week</a>, the company is announcing a preview of Project Brainwave integrated with Azure Machine Learning, which the company says will make Azure the most efficient cloud computing platform for AI.</p>
</blockquote>

<p><strong><a href="https://www.cnbc.com/2018/06/11/microsoft-hiring-engineers-for-cloud-ai-chip-design.html">Microsoft is hiring engineers to work on A.I. chip design for its cloud</a></strong></p>

<blockquote>
  <p>Microsoft is following Google's lead in designing a computer processor for artificial intelligence, according to recent job postings.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Apple"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Apple_logo.png" height="60"></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.apple.com/lae/iphone-xs/a12-bionic/">A12 Bionic The smartest, most powerful chip in a smartphone.</a></strong></p>

<blockquote>
  <p>A whole new level of intelligence. The A12 Bionic, with our next-generation Neural Engine, delivers incredible performance. It uses real-time machine learning to transform the way you experience photos, gaming, augmented reality, and more.</p>
</blockquote>

<p><br> Apple unveiled the new processor powering the new iPhone 8 and iPhone X - the A11 Bionic. <a href="https://en.wikipedia.org/wiki/Apple_A11">The A11</a> also includes dedicated neural network hardware that Apple calls a "neural engine", which can perform up to 600 billion operations per second.
<br> <a href="https://developer.apple.com/machine-learning/">Core ML</a> is Apple's current sulotion for machine learning application.</p>

<div align="center"><h3> </h3></div>

<p><a name="Alibaba"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/alibaba_logo.png" height="60"></div>

<div align="center"><h3> </h3></div>

<p>
<strong><a href="https://medium.com/syncedreview/alibabas-new-ai-chip-can-process-nearly-80k-images-per-second-63412dec22a3">Alibaba’s New AI Chip Can Process Nearly 80K Images Per Second</a></strong></p>

<blockquote>
  <p>At the Alibaba Cloud (Aliyun) Apsara Conference 2019, Pingtouge unveiled its first AI dedicated processor for cloud-based large-scale AI inferencing. The Hanguang 800 is the first semiconductor product in Alibaba’s 20-year history.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Tencent_Cloud"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Tencent_Cloud_logo.png" height="30"></div>

<div align="center"><h3> </h3></div>

<p><br />
Tencent cloud introduces <a href="https://cloud.tencent.com/product/fpga">FPGA instance</a>(Beta), with three different specifications based on Xilinx Kintex UltraScale KU115 FPGA. They will provide more choices equiped with Inter FPGA in the future.</p>

<div align="center"><h3> </h3></div>

<p><br />
<a name="Baidu"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Baidu_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.nextplatform.com/2017/08/22/first-look-baidus-custom-ai-analytics-processor/">AN EARLY LOOK AT BAIDU’S CUSTOM AI AND ANALYTICS PROCESSOR</a></p>

<blockquote>
  <p>We’ve written much over the last few years about the company’s emphasis on streamlining deep learning processing, most notably with GPUs, but <a href="https://www.nextplatform.com/2017/08/22/first-look-baidus-custom-ai-analytics-processor/">Baidu has a new processor</a> up its sleeve called the XPU. For now, the device has just been demonstrated in FPGA, but if it continues to prove useful for AI, analytics, cloud, and autonomous driving the search giant could push it into a full-bore ASIC.</p>
</blockquote>

<p><strong><a href="https://www.zdnet.com/article/baidu-creates-kunlun-silicon-for-ai/">Baidu creates Kunlun silicon for AI</a></strong></p>

<blockquote>
  <p>A pair of chips from the Chinese search giant are aimed at cloud and edge use cases. The company said it started developing a field-programmable gate array AI accelerator in 2011, and that Kunlun is almost 30 times faster. The chips are made with Samsung's 14nm process, have 512GBps memory bandwidth, and are capable of 260 tera operations per second at 100 watts.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><br />
<a name="HUAWEI"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/HUAWEI_logo.png" height="60"></div>

<div align="center"><h3> </h3></div>

<p><br />
<strong><a href="https://www.cnbc.com/2018/10/10/huawei-unveils-ai-chips-taking-aim-at-giants-like-qualcomm-and-nvidia.html">Chinese tech giant Huawei unveils A.I. chips, taking aim at giants like Qualcomm and Nvidia</a></strong></p>

<blockquote>
  <p>Huawei unveils two new artificial intelligence (AI) chips called the Ascend 910 and Ascend 310.
  The two chips are aimed at uses in data centers and internet-connected consumer devices, Rotating Chairman Eric Xu says at the Huawei Connect conference in Shanghai.
  The move pits the Chinese tech giant against major chipmakers including Qualcomm and Nvidia.</p>
</blockquote>

<p><a href="http://www.hwclouds.com/product/fcs.html">FPGA Accelerated Cloud Server</a>, high performance FPGA instance is open for beta test.</p>

<blockquote>
  <p>FPGA云服务器提供CPU和FPGA直接的高达100Gbps PCIe互连通道，每节点提供8片Xilinx VU9P FPGA，同时提供FPGA之间高达200Gbps的Mesh光互连专用通道，让您的应用加速需求不再受到硬件限制。</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Fujitsu"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Fujitsu_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<blockquote>
  <p>This <a href="https://www.nextplatform.com/2017/08/09/fujitsu-bets-deep-leaning-hpc-divergence/">DLU that Fujitsu is creating</a> is done from scratch, and it is not based on either the Sparc or ARM instruction set and, in fact, it has its own instruction set and a new data format specifically for deep learning, which were created from scratch. 
  Japanese computing giant Fujitsu. Which knows a thing or two about making a very efficient and highly scalable system for HPC workloads, as evidenced by the K supercomputer, does not believe that the HPC and AI architectures will converge. Rather, the company is banking on the fact that these architectures will diverge and will require very specialized functions. </p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Nokia"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Nokia_logo.png" height="30"></div>

<div align="center"><h3> </h3></div>

<blockquote>
  <p>Nokia has developed the <a href="https://networks.nokia.com/5g/reefshark">ReefShark chipsets</a> for its 5G network solutions. AI is implemented in the ReefShark design for radio and embedded in the baseband to use augmented deep learning to trigger smart, rapid actions by the autonomous, cognitive network, enhancing network optimization and increasing business opportunities.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Facebook"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/facebook_logo.png" height="50"></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.bloomberg.com/news/articles/2018-04-18/facebook-is-forming-a-team-to-design-its-own-chips">Facebook Is Forming a Team to Design Its Own Chips</a></p>

<blockquote>
  <p>Facebook Inc. is building a team to design its own semiconductors, adding to a trend among technology companies to supply themselves and lower their dependence on chipmakers such as Intel Corp. and Qualcomm Inc., according to job listings and people familiar with the matter.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="HPE"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/HPE_logo.png" height="80"></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.nextplatform.com/2017/11/09/hpe-developing-low-power-neural-network-chips/">HPE DEVELOPING ITS OWN LOW POWER “NEURAL NETWORK” CHIPS</a></p>

<blockquote>
  <p>In the context of a broader discussion about the company’s Extreme Edge program focused on space-bound systems, HPE’s Dr. Tom Bradicich, VP and GM of Servers, Converged Edge, and IoT systems, described a future chip that would be ideally suited for high performance computing under intense power and physical space limitations characteristic of space missions. To be more clear, he told us as much as he could—very little is known about the architecture, but there was some key elements he described.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Tesla"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Tesla_logo.png" height="60"></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.theverge.com/2019/4/22/18511594/tesla-new-self-driving-chip-is-here-and-this-is-your-best-look-yet/">Tesla’s new self-driving chip is here, and this is your best look yet</a></strong></p>

<blockquote>
  <p>...And today, at Tesla’s Autonomy Investor Day in Palo Alto, California, the company gave the world its first, detailed glimpse at what Musk is now calling “the best chip in the world” — a 260 square millimeter piece of silicon, with 6 billion transistors, that the company claims offers 21 times the performance of the Nvidia chips it was using before.</p>
</blockquote>

<p><strong><a href="https://www.theverge.com/2019/4/24/18514308/tesla-full-self-driving-computer-chip-autonomy-day-specs">Tesla’s new AI chip isn’t a silver bullet for self-driving cars</a></strong></p>

<blockquote>
  <p>Processing power is important, but building chips could be an expensive distraction for Tesla</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="LG"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/LG_logo.png" height="60"></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="http://www.lgnewsroom.com/2019/05/lg-to-accelerate-development-of-artificial-intelligence-with-own-ai-chip-2/">LG TO ACCELERATE DEVELOPMENT OF ARTIFICIAL INTELLIGENCE WITH OWN AI CHIP</a></strong></p>

<blockquote>
  <p>New AI Processor with LG Neural Engine Designed for Use in Various Products
Including Robot Vacuum Cleaners, Washing Machines and Refrigerators</p>
</blockquote>

<div align="center"><h3> </h3></div>

<div align="center"><h2><a name="IP_Vendors"></a>III. Traditional IP Vendors</h2></div>

<p><HR></p>

<div align="center"><h3> </h3></div>

<p><a name="ARM"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/ARM_logo.png" height="30"></div>

<div align="center"><h3> </h3></div>

<p><a href="http://pages.arm.com/dynamiq-technology.html">DynamIQ</a> is embedded IP giant's answer to AI age. It may not be a revolutionary design but is important for sure.<br><br>
ARM also provide a open source <a href="https://developer.arm.com/technologies/compute-library">Compute Library</a> contains a comprehensive collection of software functions implemented for the Arm Cortex-A family of CPU processors and the Arm Mali family of GPUs.<br><br>
<a href="https://developer.arm.com/products/processors/machine-learning/arm-ml-processor">Arm Machine Learning Processor</a></p>

<blockquote>
  <p>Specifically designed for inference at the edge, the ML processor gives an industry-leading performance of 4.6 TOPs, with a stunning efficiency of 3 TOPs/W for mobile devices and smart IP cameras.</p>
</blockquote>

<p><strong><a href="https://www.anandtech.com/show/12791/arm-details-project-trillium-mlp-architecture">ARM Details "Project Trillium" Machine Learning Processor Architecture</a></strong></p>

<blockquote>
  <p>Arm details more of the architecture of what Arm now seems to more consistently call their “machine learning processor” or MLP from here on now. The MLP IP started off a blank sheet in terms of architecture implementation and the team consists of engineers pulled off from the CPU and GPU teams.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Synopsys"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Synopsys_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.synopsys.com/company/newsroom/mnr/ev6x-processors-news-release.html">DesignWare EV6x Embedded Vision Processors</a><br><br>
<a href="http://mp.weixin.qq.com/s/Y4BvzmH67OaTWc_2SXIVGg">处理器IP厂商的机器学习方案 - Synopsys</a><br></p>

<div align="center"><h3> </h3></div>

<p><a name="Imagination"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Imagination_logo.png" height="60"></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.imgtec.com/powervr/vision/">PowerVR Series2NX Neural Network Accelerator</a></p>

<p><strong><a href="https://www.anandtech.com/show/12931/imagination-announces-ax2185-ax2145-powervr-2nx-neural-network-accelerators">Imagination Announces First PowerVR Series2NX Neural Network Accelerator Cores: AX2185 and AX2145</a></strong></p>

<blockquote>
  <p>the company is announcing the first products in the 2NX NNA family: the higher-performance AX2185 and lower-cost AX2145.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="CEVA"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/CEVA_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><a href="http://www.ceva-dsp.com/product/ceva-xm6/">CEVA-XM6 Fifth-generation computer vision and deep learning embedded platform</a><br><br>
<a href="http://mp.weixin.qq.com/s/rosyXJew4B0NvzY73uHz5w">处理器IP厂商的机器学习方案 - CEVA</a></p>

<p><a href="https://www.anandtech.com/show/12217/ceva-announces-neupro-neural-network-ip">CEVA Announces NeuPro Neural Network IP</a></p>

<blockquote>
  <p>Ahead of CES CEVA announced a new specialised neural network accelerator IP called NeuPro.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Cadence"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Cadence_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><a href="https://ip.cadence.com/vision&amp;CMP=TIP_BB_CDN_Vis_0501_C5_PP">Tensilica Vision DSPs for Imaging, Computer Vision, and Neural Networks</a></p>

<div align="center"><h3> </h3></div>

<p><a name="VeriSilicon"></a></p>

<div align="center"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/VeriSilicon_logo.png" height="40"></div>

<div align="center"><h3> </h3></div>

<p><a href="http://www.verisilicon.com/newsdetail_499_VivanteVIP8000.html">VeriSilicon’s Vivante VIP8000 Neural Network Processor IP Delivers Over 3 Tera MACs Per Second</a><br><br>
<a href="https://mp.weixin.qq.com/s/1W8mAMR9xaljZPLyEW0Xmw">神经网络DSP核的一桌麻将终于凑齐了</a><br></p>

<div align="center"><h3> </h3></div>

<p><a name="Videantis"></a></p>

<div align="center"><a href="http://www.videantis.com"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/videantis_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p>The <a href="http://www.videantis.com/products/deep-learning">v-MP6000UDX processor from Videantis</a> is a scalable processor family that has been designed to run high-performance deep learning, computer vision, imaging and video coding applications in a low power footprint. </p>

<div align="center"><h3> </h3></div>

<div align="center"><h2><a name="Startups_in_China"></a>IV. Startups in China</h2></div>

<p><HR></p>

<div align="center"><h3> </h3></div>

<p><a name="Cambricon"></a></p>

<div align="center"><a href="http://www.cambricon.com/en/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Cambricon_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.chinamoneynetwork.com/2018/05/04/chinese-ai-chip-maker-cambricon-unveils-new-cloud-based-smart-chip">Chinese AI Chip Maker Cambricon Unveils New Cloud-Based Smart Chip</a></strong></p>

<blockquote>
  <p>Chinese artificial intelligence chip maker Cambricon Technologies Corp Ltd has unveiled two new products, a cloud-based smart chip Cambricon MLU100 and a new version of its AI processor IP product Cambricon 1M, at a launching event in Shanghai on May 3rd.</p>
</blockquote>

<p><strong><a href="http://www.cambricon.com/index.php?c=page&amp;id=20">Cambricon release new product page, including IP, Chip and Software tools</a></strong>
<br> </p>

<p><a href="https://syncedreview.com/2017/11/06/ai-chip-explosion-cambricons-billion-device-ambition/">AI Chip Explosion: Cambricon’s Billion-Device Ambition</a></p>

<blockquote>
  <p>On November 6 in Beijing, China’s rising semiconductor company Cambricon released the Cambrian-1H8 for low power consumption computer vision application, the higher-end Cambrian-1H16 for more general purpose application, the Cambrian-1M for autonomous driving applications with yet-to-be-disclosed release date, and an AI system software named Cambrian NeuWare.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Horizon_Robotics"></a></p>

<div align="center"><a href="http://www.horizon.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Horizon_Robotics_logo.png" height="50"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.reuters.com/article/us-china-tech-semiconductors/chinese-ai-chip-maker-horizon-robotics-raises-600-million-from-sk-hynix-others-idUSKCN1QG0HW">Chinese AI chip maker Horizon Robotics raises $600 million from SK Hynix, others</a></p>

<blockquote>
  <p>Chinese chip maker Horizon Robotics said on Wednesday it had raised $600 million in its latest funding round, bringing its valuation to $3 billion, amid a push from Chinese companies and the government to boost the semiconductor industry.</p>
</blockquote>

<p>Dec. 20, <a href="http://www.horizon.ai/">Horizon Robotics</a> annouced two chip products, "Journey" for ADAS and "Sunrise" for Smart Cameras.</p>

<!-- <div align="center"><h3> </h3></div><a name="Deephi"></a><div align="center"><a href="http://www.deephi.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Deephi_logo.png" height="60"></a></div>
<div align="center"><h3> </h3></div>[DeePhi Tech](http://www.deephi.com/) has the cutting-edge technologies in deep compression, compiling toolchain, deep learning processing unit (DPU) design, FPGA development, and system-level optimization. DeePhi has the [Deep Neural Network Development Kit, DNNDK](http://deephi.com/dnndk), which is a deep learning software development kit aimed at simplifying and accelerating deep learning applications.  
This nextplatform arcicle ["FPGA Startup Gathers Funding Force for Merged Hyperscale Inference"](https://www.nextplatform.com/2017/05/22/fpga-startup-gathers-funding-force-merged-hyperscale-inference/) gave more information of the company.  
[AI startup DeePhi raises $40m in financing from Ant Financial, Samsung](http://www.chinadaily.com.cn/business/tech/2017-10/25/content_33685368.htm)
> Domestic artificial intelligent startup DeePhi Tech announced on Tuesday at a products launch that it has completed a Series A+ of financing for about $40 million.
> This round of financing was led by Alibaba's financial affiliate Ant Financial Services Group and Samsung, with China Merchants Venture and China Growth Capital participating as fellow investors.
-->

<div align="center"><h3> </h3></div>

<p><a name="Bitmain"></a></p>

<div align="center"><a href="https://www.bitmain.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Bitmain_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p>Bitcoin Mining Giant <a href="https://www.bitmain.com/">Bitmain</a> is developing processors for both training and inference tasks. </p>

<blockquote>
  <p><a href="https://qz.com/1053799/chinas-bitmain-dominates-bitcoin-mining-now-it-wants-to-cash-in-on-artificial-intelligence/">Bitmain’s newest product, the Sophon, may or may not take over deep learning</a>. But by giving it such a name Zhan and his Bitmain co-founder, Jihan Wu, have signaled to the world their intentions. The Sophon unit will include Bitmain’s first piece of bespoke silicon for a revolutionary AI technology. If things go to plan, thousands of Bitmain Sophon units soon could be training neural networks in vast data centers around the world.  </p>
</blockquote>

<p><a href="https://www.sophon.ai/">On Nov.8, Bitmain announced its Sophon BM1869 Tensor Computing Processor, Deep Learning Accelerating Card SC1 and IVS server SS1.</a></p>

<div align="center"><h3> </h3></div>

<p><a name="Chipintelli"></a></p>

<div align="center"><a href="http://www.chipintelli.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Chipintelli_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="http://www.chipintelli.com/?_l=en">Chipintelli's</a> first IC, <a href="http://www.chipintelli.com/CI1006?_l=en">CI1006</a>, is designed for automatic speech recognition application. </p>

<div align="center"><h3> </h3></div>

<p><a name="Thinkforce"></a></p>

<div align="center"><a href="http://www.think-force.com/index_e.html"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Thinkforce_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.chinamoneynetwork.com/2017/12/15/sequoia-hillhouse-yitu-technology-join-68m-series-round-chinese-ai-chip-maker-thinkforce">Sequoia, Hillhouse, Yitu Technology Join $68M Series A Round In Chinese AI Chip Maker ThinkForce</a></p>

<div align="center"><h3> </h3></div>

<p><a name="Unisound"></a></p>

<div align="center"><a href="https://www.unisound.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Unisound_logo.png" height="100"></a></div>

<p><a href="http://www.ejinsight.com/20180516-unisound-raises-us100-to-fund-ai-chip-development/">Unisound raises US$100 million to fund AI, chip development</a></p>

<div align="center"><h3> </h3></div>

<p><a name="AISpeech"></a></p>

<div align="center"><a href="http://www.aispeech.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/AISpeech_logo.png" height="100"></a></div>

<p><a href="https://medium.com/syncedreview/chinas-aispeech-raises-76m-on-advanced-speech-tech-eyes-ai-chips-2c7651ac6a8">China’s AISpeech Raises $76M on Advanced Speech Tech; Eyes AI Chips</a></p>

<div align="center"><h3> </h3></div>

<p><a name="Rokid"></a></p>

<div align="center"><a href="https://www.rokid.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Rokid_logo.png" height="30"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://technode.com/2018/06/05/rokid/">Chinese AI startup Rokid will mass produce their own custom AI chip for voice recognition</a></p>

<div align="center"><h3> </h3></div>

<p><a name="Nextvpu"></a></p>

<div align="center"><a href="http://www.nextvpu.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Nextvpu_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<blockquote>
  <p>The world leading computer vision processing IC and system company, NextVPU, today unveiled AI vision processing IC N171. N171 is the flagship IC of NextVPU’s N1 series computer vison chips. As a VPU, N171 pushes the Edge AI computing limit further from many aspects. With powerful computing engines embedded, N171 has unprecedent geometry calculation and deep neural network processing capabilities, and can be widely used in surveillance, robots, drones, UGV, smart home, ADAS applications, etc.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Canaan"></a></p>

<div align="center"><a href="https://canaan-creative.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Canaan_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<blockquote>
  <p>Canaan's <a href="https://kendryte.com/">Kendryte</a> is a series of AI chips which focuses on IoT.</p>
</blockquote>

<p><a name="Enflame"></a></p>

<div align="center"><a href="http://www.enflame-tech.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Enflame_logo.png" height="60"></a></div>

<div align="center"><h3> </h3></div>

<blockquote>
  <p>Enflame Tech is a startup company based in Shanghai, China. It was established in March 2018 with two R&D centers in Shanghai and Beijing. Enflame is developing the deep learning accelerator SoCs and software stack, targeting AI training platform solutions for the Cloud service provider and the data centers. </p>
</blockquote>

<p><a name="EEasy"></a></p>

<div align="center"><a href="http://www.eeasytech.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Eeasy_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.dealstreetasia.com/stories/chinese-tech-startups-cloudpick-zhuhai-snag-intel-capital-funding-128422/">Chinese tech startups Cloudpick, EEasy Tech snag Intel Capital funding</a></p>

<blockquote>
  <p>EEasy Technology Co. Ltd is an AI system-on-chip (SoC) design house and total solution provider. Its offerings include AI acceleration; image and graphic processing; video encoding and decoding; and mixed-signal ULSI design capabilities.</p>
</blockquote>


<p><a name="WITINMEM"></a></p>

<div align="center"><a href="http://www.witin.net/en/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/WITINMEM_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p>Founded in Oct. 2017, WITINMEM focuses on Low cost, low power AI chips and system solutions based on processing-in-memory technology in NOR Flash memory.</p>

<p><a name="Tsingmicro"></a></p>

<div align="center"><a href="http://www.tsingmicro.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Tsingmicro_logo.png" height="70"></a></div>

<div align="center"><h3> </h3></div>

<p>Qingwei Intelligent Technology (Tsing Micro) is AI chip company spin-off from Tsinghua University.</p>

<p><a name="Blacksesame"></a></p>

<div align="center"><a href="http://bst.ai/#"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Blacksesame_logo.png" height="50"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://equalocean.com/ai/20190412-black-sesame-technologies-completes-usd-100-million-series-b-financing">Black Sesame Technologies Nearly Completes 100 Million Series B Financing Round</a></strong></p>

<blockquote>
  <p>Black Sesame Technologies (黑芝麻智能科技) has nearly completed its 100 million Series B Financing round which will be used to expand cooperation with OEMs, accelerate mass production, reference design development of autopilot controllers, and software-vehicle integration.</p>
</blockquote>


<div align="center"><h3> </h3></div>

<div align="center"><h2><a name="Startups_Worldwide"></a>V. Startups Worldwide</h2></div>

<p><HR></p>

<div align="center"><h3> </h3></div>

<p><a name="Cerebras"></a></p>

<div align="center"><a href="https://www.cerebras.net/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Cerebras_logo.png" height="50"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www-wired-com.cdn.ampproject.org/c/s/www.wired.com/story/power-ai-startup-built-really-big-chip/amp">TO POWER AI, THIS STARTUP BUILT A REALLY, REALLY BIG CHIP</a></strong></p>

<blockquote>
  <p>New artificial intelligence company Cerebras Systems is unveiling the largest semiconductor chip ever built.
The Cerebras Wafer Scale Engine has 1.2 trillion transistors, the basic on-off electronic switches that are the building blocks of silicon chips. Intel’s first 4004 processor in 1971 had 2,300 transistors, and a recent Advanced Micro Devices processor has 32 billion transistors.</p>
</blockquote>

<p><strong><a href="https://venturebeat.com/2019/08/19/cerebras-systems-unveils-a-record-1-2-trillion-transistor-chip-for-ai/">Cerebras Systems unveils a record 1.2 trillion transistor chip for AI</a></strong></p>

<blockquote>
  <p>Computer chips are usually small. The processor that powers the latest iPhones and iPads is smaller than a fingernail, and even the beefy devices used in cloud servers aren’t much bigger than a postage stamp. Then there’s a new chip from startup Cerebras: It’s bigger than an iPad all by itself.
The silicon monster is almost 22 centimeters—roughly 9 inches—on each side, making it likely the largest computer chip ever, and a monument to the tech industry’s hopes for artificial intelligence. Cerebras plans to offer it to tech companies trying to build smarter AI more quickly.</p>
</blockquote>

<p><a href="https://www.cerebras.net/">Cerebras</a> is notable due to its backing from Benchmark and that its founder was the CEO of SeaMicro. It appears to have <a href="https://techcrunch.com/2016/12/19/a-stealthy-startup-called-cerebras-raised-around-25-million-to-build-deep-learning-hardware/">raised $25M</a> and remains in stealth mode.</p>

<div align="center"><h3> </h3></div>

<p><a name="Wave"></a></p>

<div align="center"><a href="http://wavecomp.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Wave_Computing_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p>Wave’s Compute Appliance is capable to run TensorFlow at 2.9 PetaOPS/sec on their 3RU appliance. Wave refers to their processors at DPUs and an appliance has 16 DPUs. Wave uses processing elements it calls Coarse Grained Reconfigurable Arrays (CGRAs). It is unclear what bit width the 2.9 PetaOPS/s is referring to. Some details can be fund in their  <a href="http://wavecomp.ai/technology/">white paper</a>.<br><br>
After HotChips 2017, in the next plateform article "<a href="https://www.nextplatform.com/2017/08/23/first-depth-view-wave-computings-dpu-architecture-systems/">First In-Depth View of Wave Computing’s DPU Architecture, Systems</a>", more details were discussed.</p>

<div align="center"><h3> </h3></div>

<p><a name="Graphcore"></a></p>

<div align="center"><a href="https://www.graphcore.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Graphcore_logo.png" height="70"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.graphcore.ai/">Graphcore</a> raised $30M of Series-A late last year to support the development of their Intelligence Processing Unit, or IPU. Resently, co-founder and Chief Technology Officer, Simon Knowles, was invited to give <a href="https://youtu.be/T8DvHnb3Y9g">a talk</a> at the 3rd Research and Applied AI Summit (RAAIS) in London, showing <a href="https://www.graphcore.ai/posts/how-to-build-a-processor-for-machine-intelligence-part-2">interesting ideas</a> behind their processor. <br><br>
In a resent post, Graphcore shows <a href="https://www.graphcore.ai/posts/preliminary-ipu-benchmarks-providing-previously-unseen-performance-for-a-range-of-machine-learning-applications">"Preliminary IPU Benchmarks"</a>  <br><br></p>

<p><strong>Simon Knowles, Graphcore CTO, spoke at the Scaled Machine Learning Conference at Stanford</strong> in March about '<a href="https://www.graphcore.ai/posts/video-scaling-throughput-processors-for-machine-intelligence">Scaling Throughput Processors for Machine Intelligence</a>'. </p>

<p><a href="https://mp.weixin.qq.com/s/CH9h8dUtoNK_2ZfkK5YU0g">解密又一个xPU：Graphcore的IPU</a> give some analysis on its IPU architecture.</p>

<div align="center"><h3> </h3></div>

<p><a name="PEZY"></a></p>

<div align="center"><a href="http://pezy.co.jp/en/index.html"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/PEZY_logo.png" height="30"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://fuse.wikichip.org/news/191/the-2048-core-pezy-sc2-sets-a-green500-record/">The 2,048-core PEZY-SC2 sets a Green500 record</a></p>

<blockquote>
  <p>The SC2 is a second-generation chip featuring twice as many cores – i.e., 2,048 cores with 8-way SMT for a total of 16,384 threads. Operating at 1 GHz with 4 FLOPS per cycle per core as with the SC, the SC2 has a peak performance of 8.192 TFLOPS (single-precision). Both prior chips were manufactured on TSMC’s 28HPC+, however in order to enable the considerably higher core count within reasonable power consumption, PEZY decided to skip a generation and go directly to TSMC’s 16FF+ Technology.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Tenstorrent"></a></p>

<div align="center"><a href="http://tenstorrent.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Tenstorrent_logo.png" height="70"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="http://tenstorrent.com/index.html">Tenstorrent</a> is a small Canadian start-up in Toronto claiming an order of magnitude improvement in efficiency for deep learning, like most. No real public details but they're are on the <a href="http://www.cogniteventures.com/the-cognitive-computing-startup-list/">Cognitive 300 list</a>.</p>

<div align="center"><h3> </h3></div>

<p><a name="Thinci"></a></p>

<div align="center"><a href="http://thinci.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Thinci_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://venturebeat.com/2018/09/05/thinci-raises-65-million-to-develop-its-ai-processors-for-automonomous-and-connected-vehicles/">ThinCI raises $65 million to develop its AI processors for automonomous and connected vehicles</a></p>

<blockquote>
  <p>Based in El Dorado Hills, California, ThinCI is developing silicon, but also software and a development kit that allows its hardware platform to be extended to a wide range of uses.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Koniku"></a></p>

<div align="center"><a href="http://koniku.io/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Koniku_logo.png" height="50"></a></div>

<div align="center"><h3> </h3></div>

<blockquote>
  <p>Founded in 2014, Newark, California startup <a href="http://koniku.io/">Koniku</a> has taken in $1.65 million in funding so far to become “the world’s first neurocomputation company“. The idea is that since the brain is the most powerful computer ever devised, why not reverse engineer it? Simple, right? Koniku is actually integrating biological neurons onto chips and has made enough progress that they claim to have AstraZeneca as a customer. Boeing has also signed on with a letter of intent to use the technology in chemical-detecting drones.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Adapteva"></a></p>

<div align="center"><a href="http://www.adapteva.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Adapteva_logo.png" height="70"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="http://www.adapteva.com/">Adapteva</a> has taken in $5.1 million in funding from investors that include mobile giant Ericsson. <a href="http://www.parallella.org/docs/e5_1024core_soc.pdf">The paper "Epiphany-V: A 1024 processor 64-bit RISC System-On-Chip"</a> describes the design of Adapteva's 1024-core processor chip in 16nm FinFet technology. </p>

<div align="center"><h3> </h3></div>

<p><a name="Knowm"></a></p>

<div align="center"><a href="http://knowm.org/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Knowm_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="http://knowm.org/">Knowm</a> is actually setup as a .ORG but they appear to be pursuing a for-profit enterprise. The New Mexcio startup has taken in an undisclosed amount of seed funding so far to develop a new computational framework called <a href="http://knowm.org/ahah-computing/">AHaH Computing</a> (Anti-Hebbian and Hebbian). The gory details can be found in <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0085175">this publication</a>, but the short story is that this technology aims to reduce the size and power consumption of intelligent machine learning applications by up to 9 orders of magnitude.</p>

<div align="center"><h3> </h3></div>

<p><a name="Mythic"></a></p>

<div align="center"><a href="https://www.mythic-ai.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Mythic_logo.png" height="20"></a></div>

<div align="center"><h3> </h3></div>

<p>A battery powered neural chip from <a href="https://www.mythic-ai.com/technology/">Mythic</a> with 50x lower power.</p>

<blockquote>
  <p>Founded in 2012, Texas-based startup Mythic (formerly known as Isocline) has taken in $9.5 million in funding with Draper Fisher Jurvetson as the lead investor. Prior to receiving any funding, the startup has taken in <a href="https://techcrunch.com/2017/03/22/mythic-launches-a-chip-to-enable-computer-vision-and-voice-control-on-any-device/">$2.5 million in grants</a>. Mythic is developing an AI chip that “puts desktop GPU compute capabilities and deep neural networks onto a button-sized chip – with 50x higher battery life and far more data processing capabilities than competitors“. Essentially, that means you can give voice control and computer vision to any device locally without needing cloud connectivity.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Kalray"></a></p>

<div align="center"><a href="http://www.kalrayinc.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Kalray_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.hpcwire.com/off-the-wire/kalray-releases-the-kalray-neural-network-3-0/">Kalray Releases the Kalray Neural Network 3.0</a> </p>
<blockquote>
  <p>Kalray (Euronext Growth Paris – ALKAL), a pioneer in processors for new intelligent systems, has announced the launch of the Kalray Neural Network 3.0 (KaNN), a platform for Artificial Intelligence application development. KaNN allows developers to seamlessly port their AI-based algorithms from well-known machine learning frameworks including Caffe, Torch and TensorFlow onto Kalray’s Massively Parallel Processor Array (MPPA) intelligent processor.</p>
</blockquote>


<div align="center"><h3> </h3></div>

<p><a name="Brainchip"></a></p>

<div align="center"><a href="http://www.brainchipinc.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Brainchip_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p>BrainChip Inc (CA. USA) was the first company to offer a <a href="http://www.brainchipinc.com/technology">Spiking Neural processor</a>, which was patented in 2008 (patent US 8,250,011). The current device, called the BrainChip Accelerator is a chip intended for rapid learning. It is offered as part of the BrainChip Studio software. BrainChip is a publicly listed company as part of BrainChip Holdings Ltd.</p>

<div align="center"><h3> </h3></div>

<p><a name="Aimotive"></a></p>

<div align="center"><a href="https://aimotive.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Aimotive_logo.png" height="60"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.bdti.com/InsideDSP/2017/07/27/AImotive">This BDTi artical</a> shows some information of aiWare IP of <a href="https://aimotive.com/what-we-do/#aiware">Aimotive</a> .</p>

<blockquote>
  <p>Speaking of chips, AImotive and partner VeriSilicon are in the process of designing a 22 nm FD-SOI test chip, which is forecast to come out of GlobalFoundries' fab in Q1 2018 (Figure 4). It will feature a 1 TMAC/sec aiWare core, consuming approximately 25 mm2 of silicon area; a Vivante VIP8000-derivative processor core will inhabit the other half of the die, and between 2-4 GBytes of DDR4 SDRAM will also be included in the multi-die package. The convolution-tailored LAM in this test chip, according to Feher, will have the following specifications (based on preliminary synthesis results):
  2,048 8x8 MACs
  Logic area (including input/output buffering logic, LAM control and MACs): 3.45mm2
  Memory (on-chip buffer): in the range of 5-25mm2 depending on configuration (10-50 Mbits).
  Another interesting activity of Aimotive is <a href="https://www.khronos.org/nnef">Neural Network Exchange Format (NNEF)</a>.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Leepmind"></a></p>

<div align="center"><a href="https://leapmind.io/en/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Leepmind_logo.png" height="30"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://leapmind.io/en/technology/">Leepmind</a> is carrying out research on original chip architectures in order to implement Neural Networks on a circuit enabling low power DeepLearning</p>

<div align="center"><h3> </h3></div>

<p><a name="Krtkl"></a></p>

<div align="center"><a href="http://krtkl.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Krtkl_logo.png" height="30"></a></div>

<div align="center"><h3> </h3></div>

<blockquote>
  <p>A crowdfunding effort for Snickerdoodle raised $224,876 and they’re currenty shipping. If you pre-order one, they’ll deliver it by summer. The palm-sized unit uses the Zynq “System on Chip” (SoC) from Xilinix.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="NovuMind"></a></p>

<div align="center"><a href="http://www.novumind.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Novumind_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<blockquote>
 
  <p>NovuMind combines big data, high-performance, and heterogeneous computing to change the Internet of Things (IoT) into the Intelligent Internet of Things (I²oT).
  Here is a <a href="https://www.novumind.com/2019/05/29/moor-insights-paper-on-novumind/">paper from Moor Insights & Strategy, a global technology analyst and research firm.</a> about NovuMind</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="REM"></a></p>

<div align="center"><a href="http://www.remicro.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/REM_logo.png" height="60"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="http://www.remicro.com/">Reduced Energy Microsystems</a> are developing lower power asynchronous chips to suit CNN inference. REM was Y Combinator's first ASIC venture according to <a href="https://techcrunch.com/2017/03/16/reduced-energy-microsystems-pits-startup-chip-chops-against-industry-giants/">TechCrunch</a>.</p>

<div align="center"><h3> </h3></div>

<p><a name="TERADEEP"></a></p>

<div align="center"><a href="https://www.teradeep.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Teradeep_logo.png" height="80"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.teradeep.com/">TeraDeep</a> is building an AI Appliance using its deep learning FPGA’s acceleration. The company claims image recognition performance on AlexNet to achieve a 2X performance advantage compared with large GPUs, while consuming 5X less power. When compared to Intel’s Xeon processor, TeraDeep’s Accel technology delivers 10X the performance while consuming 5X less power.</p>

<div align="center"><h3> </h3></div>

<p><a name="DEEP_VISION"></a></p>

<div align="center"><h2><a href="http://deepvision.io/">DEEP VISION</a></h2></div>

<div align="center"><h3> </h3></div>

<p><a href="http://deepvision.io/">Deep Vision</a> is bulding low-power chips for deep learning. Perhaps one of these papers by the founders have clues, "<a href="http://csl.stanford.edu/~christos/publications/2013.convolution.isca.pdf">Convolution Engine: Balancing Efficiency &amp; Flexibility in Specialized Computing</a>" [2013] and "<a href="http://csl.stanford.edu/~christos/publications/2015.convolution_engine.cacm.pdf">Convolution Engine: Balancing Efficiency and Flexibility in Specialized Computing</a>" [2015].</p>

<div align="center"><h3> </h3></div>

<p><a name="Groq"></a></p>

<div align="center"><h2><a href="http://groq.com/">Groq</a></h2></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.cnbc.com/2017/04/20/ex-googlers-left-secretive-ai-unit-to-form-groq-with-palihapitiya.html">Groq is founded by Ex-googlers, who designed Google TPU.</a></p>
<p><a href="groq.com">Groq's website</a> claims that its first chip will run 400 trillion operations per second with 8TOP/s per Watt power efficiency.</p>

<div align="center"><h3> </h3></div>

<p><a name="KAIST_DNPU"></p>

<div align="center"><h2>KAIST DNPU</h2></div>

<div align="center"><h3> </h3></div>

<p><a href="http://www.kaist.ac.kr/_prog/_board/?code=ed_news&amp;mode=V&amp;no=65402&amp;upr_ntt_no=65402&amp;site_dvs_cd=en&amp;menu_dvs_cd=">Face Recognition System “K-Eye” Presented by KAIST</a><p>
<p><a href="https://zhuanlan.zhihu.com/p/28328046">从ISSCC Deep Learning处理器论文到人脸识别产品</a></p>

<div align="center"><h3> </h3></div>

<p><a name="Kneron"></a></p>

<div align="center"><a href="http://www.kneron.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Kneron_logo.png" height="60"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.prnewswire.com/news-releases/kneron-to-accelerate-edge-ai-development-with-more-than-10-million-usd-series-a-financing-300556674.html">Kneron to Accelerate Edge AI Development with more than 10 Million USD Series A Financing</a></p>

<div align="center"><h3> </h3></div>

<p><a name="GTI"></a></p>

<div align="center"><a href="https://www.gyrfalcontech.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/GTI_Logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p>According to this article, <a href="https://www.prnewswire.com/news-releases/gyrfalcon-offers-automotive-ai-chip-technology-300860069.html">"Gyrfalcon offers Automotive AI Chip Technology"</a></p>

<blockquote>
  <p>Gyrfalcon Technology Inc. (GTI), has been promoting matrix-based application specific chips for all forms of AI since offering their production versions of AI accelerator chips in September 2017. Through the licensing of its proprietary technology, the company is confident it can help automakers bring highly competitive AI chips to production for use in vehicles within 18 months, along with significant gains in AI performance, improvements in power dissipation and cost advantages.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Esperanto"></p>

<div align="center"><a href="https://www.esperanto.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/esperanto_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p>According to this article, <a href="https://fuse.wikichip.org/news/686/esperanto-exits-stealth-mode-aims-at-ai-with-a-4096-core-7nm-risc-v-monster/">"Esperanto exits stealth mode, aims at AI with a 4,096-core 7nm RISC-V monster"</a></p>

<blockquote>
  <p>Although <a href="https://www.esperanto.ai/">Esperanto</a> will be licensing the cores they have been designing, they do plan on producing their own products. The first product they want to deliver is the highest TeraFLOP per Watt machine learning computing system. Ditzel noted that the overall design is scalable in both performance and power. The chips will be designed in 7nm and will feature a heterogeneous multi-core architecture.<br><br></p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="SambaNova"></a></p>

<div align="center"><a href="https://sambanovasystems.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/SambaNova_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p>According to the linkedin page of its CEO, former SPARC developer in ORACLE, <a href="https://sambanovasystems.com/">SambaNova Systems</a> is a computing startup focused on building machine learning and big data analytics platforms. SambaNova's software-defined analytics platform enables optimum performance for any ML training, inference or analytics models.<br><br>
<a href="https://techcrunch.com/2018/03/15/the-red-hot-ai-chip-space-gets-even-hotter-with-56m-for-a-startup-called-sambanova/">The red-hot AI hardware space gets even hotter with $56M for a startup called SambaNova Systems</a></p>

<blockquote>
  <p>SambaNova is the product of technology from Kunle Olukotun and Chris Ré, two professors at Stanford, and led by former Oracle SVP of development Rodrigo Liang, who was also a VP at Sun for almost 8 years.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="GreenWaves"></a></p>

<div align="center"><a href="https://greenwaves-technologies.com/en/greenwaves-technologies-2/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/GreenWaves_logo.png" height="50"></a></div>

<div align="center"><h3> </h3></div>

<blockquote>
  <p>GreenWaves Technologies develops IoT Application Processors based on Open Source IP blocks enabling content understanding applications on embedded, battery-operated devices with unmatched energy efficiency. Our first product is GAP8. GAP8 provides an ultra-low power computing solution for edge devices carrying out inference from multiple, content rich sources such as images, sounds and motions. GAP8 can be used in a variety of different applications and industries.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Lightelligence"></a></p>

<div align="center"><a href="https://www.lightelligence.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Lightelligence_logo.png" height="50"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.scientificamerican.com/article/light-powered-computers-brighten-ai-rsquo-s-future/">Light-Powered Computers Brighten AI’s Future</a></p>

<blockquote>
  <p>Optical computers may have finally found a use—improving artificial intelligence</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Lightmatter"></a></p>

<div align="center"><a href="https://www.lightmatter.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Lightmatter_logo.png" height="50"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://techcrunch.com/2018/02/05/lightmatter-aims-to-reinvent-ai-specific-chips-with-photonic-computing-and-11m-in-funding/">Lightmatter aims to reinvent AI-specific chips with photonic computing and $11M in funding</a></p>

<blockquote>
  <p>It takes an immense amount of processing power to create and operate the “AI” features we all use so often, from playlist generation to voice recognition. Lightmatter  is a startup that is looking to change the way all that computation is done — and not in a small way. The company makes photonic chips that essentially perform calculations at the speed of light, leaving transistors in the dust. It just closed an $11 million Series A.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="ThinkSilicon"></a></p>

<div align="center"><a href="https://think-silicon.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/ThinkSilicon_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://think-silicon.com/2018/02/21/1138-2/">First Low-Power AI-Inference Accelerator Vision Processing Unit From Think Silicon To Debut at Embedded World 2018</a></p>

<blockquote>
  <p>TORONTO, Canada/NUREMBERG, Germany – FEB 21st, 2018 – Think Silicon®, a leader in developing ultra-low power graphics IP technology, will demonstrate a prototype of NEMA® xNN, the world’s first low-power ‘Inference Accelerator’ Vision Processing Unit for artificial intelligence, convolutional neural networks at Embedded World 2018.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Innogrit"></a></p>

<div align="center"><a href="https://innogritcorp.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Innogrit_logo.png" height="35"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.eetimes.com/document.asp?doc_id=1334982#">Startup Puts AI Core in SSDs</a></p>
<blockquote>
  <p>Startup InnoGrit debuted a set of three controllers for solid-state drives (SSDs), including one for data centers that embeds a neural-network accelerator. They enter a crowded market with claims of power and performance advantages over rivals.</p>
</blockquote>

<p><a href="https://innogritcorp.com/technology">Innogrit Technologies Incorporated</a> is a startup seting out to solve the data storage and data transport problem in artificial intelligence and other big data applications through innovative integrated circuit (IC) and system solutions: Extracts intelligence from correlated data and unlocks the value in artificial intelligence systems; Reduces redundancy in big data and improves system efficiency for artificial intelligence applications; Brings networking capability to storage devices and offers unparalleled performance at large scales; Performs data computation within storage devices and boosts performance of large data centers.  </p>

<div align="center"><h3> </h3></div>

<p><a name="Kortiq"></a></p>

<div align="center"><a href="http://www.kortiq.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Kortiq_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="http://www.kortiq.com/">Kortiq</a> is a startup providing "FPGA based Neural Network Engine IP Core and The scalable Solution for Low Cost Edge Machine Learning Inference for Embedded Vision". Recently, they revealed some comparison data. You can also find the Preliminary Datasheet of their AIScaleCDP2 IP Core on their website.</p>

<div align="center"><h3> </h3></div>

<p><a name="Hailo"></a></p>

<div align="center"><a href="https://www.hailotech.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Hailo_logo.png" height="60"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://venturebeat.com/2019/05/14/hailo-unveils-hailo-8-an-edge-chip-custom-designed-for-ai-workloads/amp/">Hailo unveils Hailo-8, an edge chip custom-designed for AI workloads</a></p>

<blockquote>
  <p>......Hailo-8 is capable of 26 tera operations per second (TOPs) ...... In one preliminary test at an image resolution of 224 x 224, the Hailo-8 processed 672 frames per second compared with the Xavier AGX’s 656 frames and sucked down only 1.67 watts (equating to 2.8 TOPs per watt) versus the Nvidia chip’s 32 watts (0.14 TOPs per watt)......</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Tachyum"></a></p>

<div align="center"><a href="http://www.tachyum.com"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Tachyum_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.hpcwire.com/2018/05/23/silicon-startup-raises-prodigy-for-hyperscale-ai-workloads/">Silicon Startup Raises ‘Prodigy’ for Hyperscale AI Workloads</a></p>

<blockquote>
  <p>Silicon Valley-based Tachyum Inc., which has been emerging from stealth over the last year and a half, is unveiling a processor codenamed “Prodigy,” said to combine features of both CPUs and GPUs in a way that offers a purported 10x performance-per-watt advantage over current technologies. The company is primarily focused on the hyperscale datacenter market, but has aspirations to support brainier applications, noting that “Prodigy will enable a super-computational system for real-time full capacity human brain neural network simulation by 2020.”</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Alphaics"></a></p>

<div align="center"><a href="https://www.alphaics.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Alphaics_logo.png" height="50"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://www.eetimes.com/document.asp?doc_id=1333585">Startup AI Chip Passes Road Test</a></p>

<blockquote>
  <p>AlphaICs designed an instruction set architecture (ISA) optimized for deep-learning, reinforcement-learning, and other machine-learning tasks. The startup aims to produce a family of chips with 16 to 256 cores, roughly spanning 2 W to 200 W.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Syntiant"></a></p>

<div align="center"><a href="https://www.syntiant.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Syntiant_logo.png" height="30"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://semiengineering.com/syntiant-analog-deep-learning-chips/">Syntiant: Analog Deep Learning Chips</a></p>

<blockquote>
  <p>Startup Syntiant Corp. is an Irvine, Calif. semiconductor company led by former top Broadcom engineers with experience in both innovative design and in producing chips designed to be produced in the billions, according to company CEO Kurt Busch.</p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="Habana"></a></p>

<div align="center"><a href="https://habana.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Habana_logo.png" height="30"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://habana.ai/press/habana-labs-announces-gaudi-ai-training-processor/">HABANA LABS Announces Gaudi AI Training Processor</a></strong></p>
<blockquote>
  <p>TEL-AVIV, ISRAEL and SAN JOSE, CA–June 17, 2019 – Habana Labs, Ltd. (www.habana.ai), a leading developer of AI processors, today announced the Habana Gaudi™ AI Training Processor. Training systems based on Gaudi processors will deliver an increase in throughput of up to four times over systems built with equivalent number GPUs.</p>
</blockquote>

<p><strong><a href="https://habana.ai/media-center/">You can also find the reports in the media</a></strong></p>

<p><a href="https://www.eetimes.com/document.asp?doc_id=1333719">Startup’s AI Chip Beats GPU</a></p>

<blockquote>
  <p>The Goya chip can process 15,000 ResNet-50 images/second with 1.3-ms latency at a batch size of 10 while running at 100 W. That compares to 2,657 images/second for an Nvidia V100 and 1,225 for a dual-socket Xeon 8180. At a batch size of one, Goya handles 8,500 ResNet-50 images/second with a 0.27-ms latency.</p>
</blockquote>


<div align="center"><h3> </h3></div>

<p><a name="aiCTX"></a></p>

<div align="center"><a href="https://aictx.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/aiCTX_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.eetimes.com/document.asp?doc_id=1333983">Baidu Backs Neuromorphic IC Developer</a></strong></p>

<blockquote>
  <p>MUNICH — Swiss startup aiCTX has closed a $1.5 million pre-A funding round from Baidu Ventures to develop commercial applications for its low-power neuromorphic computing and processor designs and enable what it calls “neuromorphic intelligence.” It is targeting low-power edge-computing embedded sensory processing systems.</p>
</blockquote>

<p><a name="Flexlogix"></a></p>

<div align="center"><a href="http://www.flex-logix.com/nmax"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/flexlogix_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.zdnet.com/article/ai-startup-flex-logix-targets-nvidias-market-for-inference-chips/">AI startup Flex Logix touts vastly higher performance than Nvidia</a></strong></p>

<blockquote>
  <p>Four-year-old startup Flex Logix has taken the wraps off its novel chip design for machine learning. CEO Geoff Tate describes how the chip may take advantage of an "explosion" of inferencing activity in "edge computing," and how Nvidia can't compete on performance.</p>
</blockquote>

<p><a name="PFN"></a></p>

<div align="center"><a href="https://projects.preferred.jp/mn-core/en/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/PFN_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.preferred-networks.jp/en/news">Preferred Networks develops a custom deep learning processor MN-Core for use in MN-3, a new large-scale cluster, in spring 2020</a></strong></p>

<blockquote>
  <p>Dec. 12, 2018, Tokyo Japan – Preferred Networks, Inc. (“PFN”, Head Office: Tokyo, President & CEO: Toru Nishikawa) announces that it is developing MN-Core (TM), a processor dedicated to deep learning and will exhibit this independently developed hardware for deep learning, including the MN-Core chip, board, and server, at the SEMICON Japan 2018, held at Tokyo Big Site. 
 </p>
</blockquote>

<p><a name="Cornami"></a></p>

<div align="center"><a href="http://cornami.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Cornami_logo.jpg" height="30"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.zdnet.com/article/ai-startup-cornami-reveals-details-of-neural-net-chip/">AI Startup Cornami reveals details of neural net chip</a></strong></p>

<blockquote>
  <p>Stealth startup Cornami on Thursday revealed some details of its novel approach to chip design to run neural networks. CTO Paul Masters says the chip will finally realize the best aspects of a technology first seen in the 1970s. 
 </p>
</blockquote>

<p><a name="Anaflash"></a></p>

<div align="center"><a href="http://anaflash.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Anaflash_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.smart2zero.com/news/ai-chip-startup-offers-new-edge-computing-solution">AI chip startup offers new edge computing solution</a></strong></p>

<blockquote>
  <p>Anaflash Inc. (San Jose, CA) is a startup company that has developed a test chip to demonstrate analog neurocomputing taking place inside logic-compatible embedded flash memory. 
 </p>
</blockquote>

<p><a name="Optalysys"></a></p>

<div align="center"><a href="https://www.optalysys.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Optalysys_logo.png" height="40"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.globenewswire.com/news-release/2019/03/07/1749510/0/en/Optalysys-launches-world-s-first-commercial-optical-processing-system-the-FT-X-2000.html">Optalysys launches world’s first commercial optical processing system, the FT:X 2000</a></strong></p>

<blockquote>
  <p>Optalysys develops Optical Co-processing technology which enables new levels of processing capability delivered with a vastly reduced energy consumption compared with conventional computers. Its first coprocessor is based on an established diffractive optical approach that uses the photons of low-power laser light instead of conventional electricity and its electrons. This inherently parallel technology is highly scalable and is the new paradigm of computing. 
 </p>
</blockquote>

<p><a name="etacompute"></a></p>

<div align="center"><a href="https://etacompute.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/etacompute_logo.png" height="80"></a></div>

<div align="center"><h3> </h3></div>

<p><a href="https://spectrum.ieee.org/tech-talk/semiconductors/processors/eta-compute-debuts-spiking-neural-network-chip-for-edge-ai">Eta Compute Debuts Spiking Neural Network Chip for Edge AI</a></p>

<blockquote>
  <p>Chip can learn on its own and inference at 100-microwatt scale, says company at Arm TechCon.</p>
</blockquote>

<p><a name="Achronix"></a></p>

<div align="center"><a href="https://www.achronix.com/product/speedster7t/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Achronix_logo.png" height="30"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.eetimes.com/document.asp?doc_id=1334717">Achronix Rolls 7-nm FPGAs for AI</a></strong></p>

<blockquote>
  <p>Achronix is back in the game of providing full-fledged FPGAs with a new high-end 7-nm family, joining the Gold Rush of silicon to accelerate deep learning. It aims to leverage novel design of its AI block, a new on-chip network, and use of GDDR6 memory to provide similar performance at a lower cost than larger rivals Intel and Xilinx.</p>
</blockquote>

<p><a name="Areanna"></a></p>

<div align="center"><a href="https://areanna-ai.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Areanna_logo.png" height="60"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.eetimes.com/document.asp?doc_id=1334947#">Startup Runs AI in Novel SRAM</a></strong></p>

<blockquote>
  <p>Areanna is the latest example of an explosion of new architectures spawned by the rise of deep learning. The debut of a whole new approach to computing has fired imaginations of engineers around the industry hoping to be the next Hewlett and Packard.</p>
</blockquote>

<p><a name="Neuroblade"></a></p>

<div align="center"><a href="https://www.neuroblade.ai/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Neuroblade_logo.png" height="30"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.eetasia.com/news/article/NeuroBlade-Preps-Inference-Chip">NeuroBlade Preps Inference Chip</a></strong></p>

<blockquote>
  <p>Add NeuroBlade to the dozens of startups working on AI silicon. The Israeli company just closed a $23 million Series A, led by the founder of Check Point Software and with participation from Intel Capital.</p>
</blockquote>

<p><a name="Luminous"></a></p>

<div align="center"><a href="https://www.luminouscomputing.com/"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Luminous_logo.png" height="90"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.technologyreview.com/s/613668/ai-chips-uses-optical-semiconductor-machine-learning/">Bill Gates just backed a chip startup that uses light to turbocharge AI</a></strong></p>

<blockquote>
  <p>Luminous Computing has developed an optical microchip that runs AI models much faster than other semiconductors while using less power.</p>
</blockquote>

<p><a name="Efinix"></a></p>

<div align="center"><a href="https://www.efinixinc.com"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/Efinix_logo.png" height="25"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://www.zdnet.com/article/chip-startup-efinix-hopes-to-bootstrap-ai-efforts-in-iot/">Chip startup Efinix hopes to bootstrap AI efforts in IoT</a></strong></p>

<blockquote>
  <p>Six-year-old startup Efinix has created an intriguing twist on the FPGA technology dominated by Intel and Xiliinx; the company hopes its energy-efficient chips will bootstrap the market for embedded AI in the Internet of Things.</p>
</blockquote>

<p><a name="AIstorm"></a></p>

<div align="center"><a href="https://aistorm.ai"><img src="https://github.com/basicmi/Deep-Learning-Processor-List/raw/master/resource/AIstorm_logo.png" height="60"></a></div>

<div align="center"><h3> </h3></div>

<p><strong><a href="https://venturebeat.com/2019/02/11/aistorm-raises-13-2-million-for-ai-edge-computing-chips/">AIStorm raises $13.2 million for AI edge computing chips</a></strong></p>

<blockquote>
  <p>David Schie, a former senior executive at Maxim, Micrel, and Semtech, thinks both markets are ripe for disruption. He — along with WSI, Toshiba, and Arm veterans Robert Barker, Andreas Sibrai, and Cesar Matias — in 2011 cofounded AIStorm, a San Jose-based artificial intelligence (AI) startup that develops chipsets that can directly process data from wearables, handsets, automotive devices, smart speakers, and other internet of things (IoT) devices. </p>
</blockquote>

<div align="center"><h3> </h3></div>

<p><a name="AIChipCompilers"></a></p>

<div align="center"><h2>AI Chip Compilers</h2></div>

<p><HR>
1. <a href="https://github.com/pytorch/glow">pytorch/glow</a><br>
2. <a href="https://tvm.ai/">TVM:End to End Deep Learning Compiler Stack</a><br>
3. <a href="https://www.tensorflow.org/xla">Google Tensorflow XLA</a></strong><br>
4. <a href="https://developer.nvidia.com/tensorrt">Nvidia TensorRT</a><br>
5. <a href="https://github.com/plaidml/plaidml">PlaidML</a><br>
6. <a href="https://github.com/NervanaSystems/ngraph">nGraph</a><br>
7. <a href="https://github.com/Tiramisu-Compiler/tiramisu">MIT Tiramisu compiler</a><br>
8. <a href="https://onnc.ai/">ONNC (Open Neural Network Compiler)</a><br>
9. <a href="https://github.com/tensorflow/mlir/">Multi-Level Intermediate Representation</a><br>
10. <a href="http://tensor-compiler.org/">The Tensor Algebra Compiler (taco)</a><br>

<div align="center"><h3> </h3></div>

<p><a name="AIChipBenchmarks"></a></p>

<div align="center"><h2>AI Chip Benchmarks</h2></div>

<p><HR>

1. <a href="https://dawn.cs.stanford.edu/benchmark/index.html">DAWNBench:An End-to-End Deep Learning Benchmark and Competition Image Classification (ImageNet)</a><br>
2. <a href="https://github.com/rdadolf/fathom">Fathom:Reference workloads for modern deep learning methods</a><br>
3. <a href="https://mlperf.org/">MLPerf:A broad ML benchmark suite for measuring performance of ML software frameworks, ML hardware accelerators, and ML cloud platforms</a>. You can find MLPerf v0.5 results <a href="https://mlperf.org/results/">here.</a>. <strong><a href="https://mlperf.org/inference-overview/">MLPerf Inference Benchmarks is here</a></strong>.<br>
4. <a href="https://aimatrix.ai/en-us/index.html">AI Matrix</a><br>
5. <a href="http://ai-benchmark.com/index.html">AI-Benchmark</a><br>
6. <a href="https://github.com/AIIABenchmark/AIIA-DNN-benchmark">AIIABenchmark</a><br>
7. <a href="https://www.eembc.org/mlmark/">EEMBC MLMark Benchmark</a><br>

<div align="center"><h3> </h3></div>

<p><a name="Reference"></a></p>

<div align="center"><h2>Reference</h2></div>

<p><HR>

<div align="center"><h3> </h3></div>
      
1. <a href="https://meanderful.blogspot.jp/2017/06/fpgas-and-ai-processors-dnn-and-cnn-for.html">FPGAs and AI processors: DNN and CNN for all</a><br>
2. <a href="http://www.nanalyze.com/2017/05/12-ai-hardware-startups-new-ai-chips/">12 AI Hardware Startups Building New AI Chips</a><br>
3. <a href="http://eyeriss.mit.edu/tutorial.html">Tutorial on Hardware Architectures for Deep Neural Networks</a><br>
4. <a href="https://nicsefc.ee.tsinghua.edu.cn/projects/neural-network-accelerator/">Neural Network Accelerator Inference</a><br>
5. <strong>"White Paper on AI Chip Technologies 2018". You can download it from <a href="https://cloud.tsinghua.edu.cn/f/9aa0a4f0a5684cc48495/?dl=1">here</a>, or <a href="https://drive.google.com/open?id=1ieDm0bpjVWl5MnSESRs92EcmoSzG5vcm">Google drive.</a></strong><br>

<div align="center">
<a href="http://www.reliablecounter.com" target="_blank"><img src="http://www.reliablecounter.com/count.php?page=https://basicmi.github.io/Deep-Learning-Processor-List/&digit=style/plain/3/&reloads=1" alt="laptop" title="laptop" border="0"></a>
</div>

